[["index.html", "Best Coding Practices for R CoverPage", " Best Coding Practices for R Vikram Singh Rawat 2021-04-09 CoverPage Know the rules well, so you can break them effectively.  The Dalai Lama "],["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction The most damaging phrase in the language is: Its always been done that way.*.  Grace Hopper Did you try to read the title from the cover of the book? I could have done a thousand things to make it easier for you to read it. But this cover reminds me of how often we overlook simple things which are very crucial from the readers point of view. R is an excellent programming language its turing complete and doesnt lack anything for a production level code. It can be used in the entire data domain from APIs to dashboards to apps and much more. Trust the language and trust in yourself. Its a journey everyone has gone through and everyone must go through. R programmers have a bad reputation for not writing production level code. It stems from the fact that we mostly arent trained programmers. We tend to overlook things that are crucial from a programming standpoint. As R programmers we are often less inclined to write the code for production. Mostly we try to write scripts and when we are asked to deploy the same we just wrap it in a function and provide it to the IT team. I have been at the receiving end of these issues where I had to maintain a poorly written code; columns were referred by numbers, functions were dependent upon global environment variables, 50+ lines functions without arguement, poor naming conventions etc. I too am a self taught programmer and have gone through these hiccups of code deployment, code reviews and speed issues. World going forward will all be code and data. The sooner you learn these skills the better it is for you to have trust in your own programming skills. R is a huge language and I would like to share the little knowledge I have in the subject. I dont claim to be an expert but this book will guide you in the right path wherever possible. Most of the books about R programming language will tell you what are the possible ways to do one thing in R. This book will only tell you one way to do that thing correctly. I will try to write it as a dictionary as succinctly as possible. So that you can use it for references. Let the journey begin "],["folder.html", "Chapter 2 Folder Structure 2.1 Organizing files 2.2 Create Projects 2.3 Naming files 2.4 Folders Based on File-Type 2.5 Creating Sub-folders 2.6 Conclusion", " Chapter 2 Folder Structure 2.1 Organizing files The best way to organize your code is to write a package. Organizing your code is the first and foremost thing you should learn. Because as the project grows and multiple files are put into a folder it gets harder to navigate the code. A proper folder structure definitely helps in these times. I Just couldnt emphasis it enough that best way to organize your code is to write a package. But even when you are not planning to write a package. There are best practices to make it readable and make a smooth navigation. 2.2 Create Projects Its such a minor thing to say but I still till date see code like this: setwd(&quot;c://myproject_name/&quot;) It was a good practice like 5 - 6 years ago. Now Rstudio has a feature to create project. new project Once you create a project it is easier to manage your files and folders and its easier to give it somebody as well. It has virtually the same effect but then you can use Rstudio a little better. Its something I recommend to every user regardless of the skill level. 2.3 Naming files I data science most common problem is that we dont change the file names of excel or csv files provided by business people. And most of the time those file names are totally abbreviated with spaces in between and multiple cases like Total Sales Mike 202002-AZ1P2R.csv. This name is useful for the MIS or Business Analyst as they have a different way of organizing files then yours. They might do it because they have to keep a record of different people and have to provide it anytime asked. But as a Data Scientist your work is entirely different. You are not delivering files you are writing code. Let me reiterate this fact YOU ARE WRITING CODE. In most of the scenarios Data Science is more like programming less like science. Even though it has proportion of both of them. Using fundamentals of programming practices will help you out in long term. So change such file names to sales_data_mike_feb2020.csv or something similar. There are no right or wrong names just what makes more sense to a new user. There is a trick about naming conventions: use all lower case or upper case ( helps you in never forgetting the cases ) use underscore in between ( Because file names are mostly long Camel Or Pascal cases may confuse users) make the name as general as possible ( make sure a newcomer should be able to understand it without any problem) In choosing a name there are no wrong answers only confusing ones 2.4 Folders Based on File-Type A Very common practice is to keep different file types in different folder. One of the main mistake I see people writing code like this. DBI::dbGetQuery(conn, &quot; select count(*) as numbers, max(colname) as maxSome, min(colname) as minSome, from tablename group by col1, col2, col3 order by numbers &quot;) or codes like this. shiny::HTML( &quot; &lt;p&gt;At Mozilla, were a global community of&lt;/p&gt; &lt;ul&gt; &lt;!-- changed to list in the tutorial --&gt; &lt;li&gt;technologists&lt;/li&gt; &lt;li&gt;thinkers&lt;/li&gt; &lt;li&gt;builders&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;working together to keep the Internet alive and accessible, so people worldwide can be informed contributors and creators of the Web. We believe this act of human collaboration across an open platform is essential to individual growth and our collective future.&lt;/p&gt; &lt;p&gt;Read the &lt;a href=\\&quot;https://www.mozilla.org/en-US/about/manifesto/\\&quot;&gt;Mozilla Manifesto&lt;/a&gt; to learn even more about the values and principles that guide the pursuit of our mission.&lt;/p&gt; &quot; ) This is a bad coding style. Every time I see this type of code I realize that the person doesnt believe that either the code will change or It will be extended. There is nothing permanent in the programming neither code, nor frameworks and not even languages. If you keep this type of code in separate SQL files or html files you can easily edit them later, code will be more easier to read and there will be a separation of concern. Tomorrow if you need help in SQL or HTML, a UI designer or a Database designer can look into your code without getting bogged down in R code. It makes bringing more people to the team easier. 2.5 Creating Sub-folders On bigger projects simple folder structure tend to become more confusing. This is the main concern I have with the data folder every data scientist create and put all the files he has in that single folder. In these scenarios its better to have a sub-folder for different file types or may be different roles. Like you can create sub-folders based on file-types like CSVs, json, rds etc.. or you can even create sub-folders based on roles or needs Like all the data related to one tab or one functionality goes in one folder and so on There has to be a logical consistency in the folder structure. Its primarily for you to not get lost in your own folders that you created and secondary for people working with you to understand your code and help in places you need help. 2.6 Conclusion You have to create folders and everything has to be arranged in. Keep everything as organized as you keep your house. There are a certain principles that will help you in it. Create projects Name the files properly Create a file for different language create sub-folders wherever you fill necessary. "],["code.html", "Chapter 3 Code Structure 3.1 Create Sections 3.2 Order of Code 3.3 Indentation 3.4 Conclusion", " Chapter 3 Code Structure Once you have arranged the files and folders in a logical way then comes the fact that the code itself should be arranged in such a way that it feels easy to go through. Always remember Code is read more often then its written. Your world should revolve around this line. If you delegate your work while leaving your firm to someone else than the person who is handling your code should be able to understand everything you were trying to do. You will be in that position someday and you would wish your colleagues must have done the same. Even if you arent sharing your code to somebody one day when you will return back to the project after say 7 to 8 months you will be surprised to see the mess you created back then. With this in mind hope this will help you in your journey. 3.1 Create Sections Rstudio gives you an ability to create section by pressing ( ctrl + shift + R ), or you can create one by adding 4 dashes (-) after a comment or 4 hash symbol (#) after a comment. # some comment ---- # some comment #### Both are valid syntax. you can name your section in the comment. Same rule applies for the Rmarkdown documents as well you should always name your code chunk. # ```{r chunkname, cache=TRUE} It also helps you jump between sections (Shift+Alt+J). You can easily switch between sections and fold them at will. It helps you not only in navigation but keeping a layout of the entire code as well. A 800+ line files will look something like this. code chunk It makes your code beautiful to look and makes it maintainable in long run. 3.2 Order of Code When you write code there are standard practices that are used across the domain and you should definitely use them. These are simple rules that most beginners arent concerned about but the more experience you gain the more you start to realize the latent power of code organization. Here are a simple tip you should use. Call your libraries on top of code Set all default variables or global options and all the path variables at the top of the code. Source all the code at the beginning Call all the data-files at the top In this exact order. This coherence keeps all your code easy to find. Most annoying thing in debugging someone else code is finding path variables laid out inside functions. Please dont ever do that. That is not a good practices. Take a look at one of my file code order If you think you will forget it. There is a golden rule you must remember. Put all the external dependencies on top of your code. Everything that I mentioned above is external to the code in the file. In exact order. Libraries are external to the file. path variables other files apart from one you are working is external as well. databases and CSV Just by doing this you will be able to navigate better in your code. There arent any hard and fast rules for this only logical and sensible ones. Feel free to come up with your own layout that helps you in your analysis journey. 3.3 Indentation It goes without saying that indentation makes your code readable. Python is not the only language who has the luxury of indentation. No matter what language you work in your code should be properly indented so that we can understand the nature of code written. There are a few things you can understand about indentation. Maintain same number of spaces throughout your code. Your editor will help you out with it for sure but even if you are working on multiple editors. If you choose 2 spaces or 4 spaces as an equivalent of tabs you should stick to it. This is a golden rule you should never break. Then maintain the same style in your code. Look at the code below. foo &lt;- function( first_arg, second_arg, third_arg ){ create_file &lt;- readxl::read_excel(path = first_arg, sheet = second_arg, range = third_arg) } bar &lt;- function( first_arg, second_arg, third_arg ){ create_file &lt;- readxl::read_excel( path = first_arg, sheet = second_arg, range = third_arg ) } function foo is written horizontal and bar is written vertical. I would prefer styling of function bar but you may choose one and stick to it for entire project. Mixing styles is not considered good and might create problem in code review. There is a package by name grkstyle which implements vertical arrangement of code as mentioned above. You can look into it as well. 3.4 Conclusion In this chapter we discussed how to structure your code to make it more meaningful to read and easier to debug. The key takeaways from this chapter is: Create sections to write beautiful and navigable code Put those sections in a logical order Dont "],["func.html", "Chapter 4 Functions 4.1 Metadata or Information header 4.2 Pass everything through parameters 4.3 Use Return Statement 4.4 Keep a consistency in Return Type 4.5 Use Sensible Names for parameters too 4.6 use tryCatch 4.7 Write simple and unique functions 4.8 Dont load libraries or source code inside a function 4.9 Use Package::Function() approach 4.10 Conclusion", " Chapter 4 Functions I can not over emphasize the importance of functions. As a data scientist most of the time you will be writing functions. Only in couple of cases where you have to write complicated classes there too methods are nothing more than functions. Having solid grasp of best practices in functions is a must for everybody working in any language what-so-ever. Hopefully this chapter will help you in best coding practices for functions. 4.1 Metadata or Information header As I mentioned in the previous chapter it is a good practice to create sections for everything you do in R. functions are no exception to the rule. But along with that there are a couple of information you should write along with the function. I worked in a few MNC where we had to write metadata of every function before writing it down. It makes it easier for code-reviewer to understand you code and for the entire team to collaborate in the project. Its good for personal projects too Let me give you an example of what I mean by this. functions metadata You can see that if you are working on large teams or may be in big corporate settings where anybody can be reassigned to a different project. This data helps by identifying who wrote what and why. Examples of some important tags can be : written by written on parameters modified by modified on purpose descriptions You can create your own tags based on usecases and information needed for further scenarios. 4.2 Pass everything through parameters I have seen people writing functions with calling things from global environments. Take a look at the code below. foo &lt;- function(x){ return( x + y) } y &lt;- 10 foo(5) ## [1] 15 Here the value of foo is based on y which is not a part of the function instead its in global environment and function always have to search global environment for the object. consider these scenarios: bar &lt;- function(x, y){ y &lt;- y return( foo(x) ) } bar(5, 20) ## [1] 15 you would assume that the answer is 25 but its 15 because foo was created in the global environment and it will always look up value in global environment before anything else. This is called Lexical Scoping its okay if you dont know it. It is very confusing and could mess up your code at any point in time. I am an experienced R programmer I too have trouble getting my head around it. We can avoid all these situations by following the best coding practices that have been used in software industries for years. Function should be a self contained code which shouldnt be impacted by the outer world. Only is certain scenarios you allow to deviate from these rules but its a good coding practice none the less. now in the above example instead of relying on the global variable if I just had created a parameter for Y, my code would be simpler to write and easier to understand and I would not have to think about lexical scoping on every step. foo &lt;- function( x, y ){ return( x + y ) } bar &lt;- function( x, y ){ return( foo( x, y ) ) } bar(5, 20) ## [1] 25 Now this code returns 25 as we all expected and trust me the Y is still available in global environment but that doesnt impact the foo or bar at all. Now you can nest this function under multiple other functions and it will behave exactly like it should. There is a golden rule you should take away from this section. Avoid Global Variables at all costs. As much as possible pass everything through the parameters. That what they are for right !!! 4.3 Use Return Statement It is a very simple thing yet most of the R users never worry about it because R takes care of finer details for you. But return statements actually make your code easier to read. Suppose you have to review code return statement makes it easier to glance at the code and understand what is it doing. Almost all the programming languages are habitual with it. There are no good advantage I can tell you for a return statement other than readability. But just by following these practices R community as a whole could get more respect in programming community. So please use Return statements wherever possible. In Big MNC your code will never pass reviewer unless it has return statements. It also is good for functions that dont return anything you can just return true or false depending on the fact that the function ran without producing any error. Functions where you modify a data.table or where you change something in the database etc Its a standard practice in old programming languages like C++ and its a good practice indeed. We as a community should embrace these practices which will help us down the road. 4.4 Keep a consistency in Return Type Return type of a function should be consistent regardless of what happens in a code. You may assume this is so simple that it goes without saying who would in their sane mind return character vector instead of a numerical one and you would be right. But Things get complicated when people start to work in composite data types like Lists and Dataframes. Working with lists people get confused and forget this basic principle. I have seen function returning list of 2 elements on some conditions and 3 on other and 4 on some more. It makes it harder for users to work on those return values. Dont even get me started on dataframes. People write functions that do some magic stuff on dataframes and it sometimes return a dataframe of 10 columns, sometime 11 and sometime 8. Its such a common mistake to make. I understand if you are fetching a table from database and returning that same table via functions but during manipulations you must add empty columns or delete existing ones to make it consistent for the end user regardless of the conditions you have in the functions. 4.5 Use Sensible Names for parameters too Yet another simple thing but because most of us including me come from non computer science background we have a tendency to use names like x, y, z, beta, theta, gamma, string etc in our function parameters. I too am guilty of doing it in above code for foo and bar functions and in general. Many good and well established libraries in R are guilty of this sin too But in long run these words dont make much sense. Its hard to maintain that code and its hard for user as well. Lets take an example : join &lt;- function(x, y) x + y join(x = 12, y = 12) ## [1] 24 do you see that as a user who hasnt written or even looked at the code its already hard for him to understand what does x and y stands for. Only to get an error like this. join(x = &quot;mtcars&quot;, y = &quot;iris&quot;) ## Error in x + y: non-numeric argument to binary operator I know it is a stupid example but I see it every time in real code. When you only need numeric values why not include that information in the parameter name. something like: join &lt;- function(num_x, num_y) num_x + num_y It may not seem like much but this small change makes the life of the user so much better where he doesnt need to consult the documentation again and again. Their are other ways you can come up with sensible names in your code just to avoid this issue. Its a standard practice during code review to check the names and these names are never allowed in production environment. We will discuss more about names in another chapter but for now understand that parameter names are just as important as the name of the function and it should be meaningful and easier to understand. There should be some information buried in the name. 4.6 use tryCatch During deployment we would not like the shiny app or rest api or the chron job to fail. Its not a good experience to have for either the developer or the client. Best way to avoid it is wrap every function in a tryCatch block and log the errors. This way if you app has some bugs ( which every app does ). It will not crash and not destroy the experience of all the other people using it. Lets bring back the foo function : foo &lt;- function( x, y ){ tryCatch( expr = { return( x + y ) }, error = function(e){ print( sprintf(&quot;An error occurred in foo at %s : %s&quot;, Sys.time(), e) ) }) } foo(&quot;mtcars&quot;, &quot;iris&quot;) ## [1] &quot;An error occurred in foo at 2021-04-09 10:29:50 : Error in x + y: non-numeric argument to binary operator\\n&quot; Now imagine this line to be printed in a json file or inserted in a database with time stamp and other information instead of crashing the entire code only a particular functionality will not run which is huge. This is the difference between staying late on Saturday night to fix a bug vs telling them that I will fix it on Monday. To me that is big enough. 4.7 Write simple and unique functions Task of one function should be to do one thing and one thing only. There are numerous times when people assume they have written excellent code because everything is in a function. Purpose of a function is to reduce one unique task in a single line. If your function does multiple things then its a good Idea to Break your function into multiple one and then create a function which uses all of them. average_func &lt;- function( mult_params ){ tryCatch( expr = { ### # code to do stuff 1 ### ### # code to do stuff 2 ### }, error = function(e){ ### # code to log errors ### }) } Now imagine if today you are logging on a json file and tomorrow client wants to log it into a database. Changing it on every function is not only time consuming but dangerous in terms that now you can break the code. Now compare that to this code. stuff_1 &lt;- function(params_1){ ### # code to do stuff 1 ### } stuff_2 &lt;- function(params_2){ ### # code to do stuff 1 ### } log_func &lt;- function( log_params){ ### # code to log errors ### } best_func &lt;- function( mult_params ){ tryCatch( expr = { stuff_1() stuff_2() }, error = function(e){ log_func() }) } Here in this code every function has a clear responsibility and the main function is just a composite of multiple unique functions and it will be very easy to debug this code or change the functionality entirely. 4.8 Dont load libraries or source code inside a function You may assume nobody does it. But I have seen people doing it times and times again. foo &lt;- function( x ){ library(data.table) setDT(x) } bar &lt;- function( x ){ source(file = &quot;&quot;) } In functional programming terminology these functions are called impure functions . Function which change the global environment or some persistent changes are called impure functions. They require very delicate handling of the entire project. If I dont know how many packages and what version of them am I dependent on or what files have I loaded in my environment it makes debugging the code a lot more harder. By following this style of code you are making the debugging harder for your project. In fact I would argue that you should remove all the external dependencies from a function. Take this code for example. foo &lt;- function(x){ exl &lt;- readxl::read_excel(path = &quot;increased/external/dependency&quot;) ## ### do some data operation in the function ## return(exl) } bar &lt;- function( x, filepath = &quot;increased/external/dependency&quot;){ exl &lt;- readxl::read_excel(path = filepath) ## ### do some data operation in the function ## return(exl) } Foo and bar are both doing the same thing and both are relying on an external path for code to work. But because bar is clearly stating the filepath as an argument it is easier to change and adapt to new needs. If you still need to rely on a package call it from the main script not from a function. And if you absolutely need some functions of a package inside a particular function then use qualified imports and dont load the entire package. 4.9 Use Package::Function() approach R classes work differently than the traditional oops we all are aware of. Instead of object_of_class.Method syntax like other programmings have, we in R use method( object_of_class ) syntax. Where just by changing name collision is a pretty common thing. Its a pretty common thing in R that 2 packages use same function name for different operations. So Its always better to use qualified imports fancy name for mentioning which package does the function comes from. 4.9.1 You should load libraries in the order of their usage # library(&quot;not_used_much&quot;) # library(&quot;least_used&quot;) # library(&quot;fairly_used&quot;) # library(&quot;most_used&quot;) # library(&quot;cant_do_without_it&quot;) R uses the loading sequence to identify which function to give preference. Its usually to the last package loaded. Its called masking and its not a reliable technique but its better to arrange your code in that order for sake of simplicity. and Yes do not forget to mention the package name clearly. like prefer writing this always: # dplyr::filter() # stats::filter() # # ## instead of # # filter() For your small project this might not be a big deal but when multiple people are working on a code everybody might not be familiar with the packages you are using and they might not know that there is a naming collision between 2 functions. Its a best practice to explicitly tell R that this function comes from this package. It saves a lot of your time and for the person who is going to maintain your code too And it makes your debug experience a little better. 4.10 Conclusion In this chapter we discussed the best practices for writing functions in R. Here are the key takeaways from the chapter. write information about the function at top of it. avoid global variable and pass everything through parameters use return statement to end your function keep consistency in return types of a function use logical names for parameter use tryCatch in every function functions are supposed to do one thing and one thing only dont try to change global environment without letting the user know Use qualified imports with syntax package::functions every time possible "],["names.html", "Chapter 5 Naming Conventions 5.1 Popular naming conventions 5.2 Informative Names 5.3 Conclusions", " Chapter 5 Naming Conventions This chapter is crucial only for people to understand what are the bad naming practices we the R users have acquired over the years because of flexibility in the language. These names we give to the data or variables are not valid outside or R community and thus are subject to code reviews. You may even be asked to change name before deploying the code in production. The more bad naming practices the more time it takes you to fix them. Its a good practice to know the best practices for naming things in general. 5.1 Popular naming conventions There are 3 most famous naming conventions in programming community. They are used throughout the code in big projects to smaller ones. These are : 5.1.1 camelCase These names start with small letter and every subsequent word will start with upperCase letter like my name in camelCase would be written as vikramSinghRawat. All the functions in SHINY are camelCase. Its a great example of camelCase naming conventions. 5.1.2 PascalCase PascalCase is just like camel case but the only difference is the first letter is also UpperCase. My name would be written as VikramSinghRawat. 5.1.3 snake_case These names are all lower case with underscore between the name. My name in snake_case would be vikram_singh_rawat. TIDYVERSE is a great example of snake_cases. I really like the naming conventions of packages like stringi and quanteda. whenever you start a project you should choose one of the naming conventions for the entire team. So that no matter who writes the code there is a logical consistency in the names and anybody can predict the next letter. In many projects that I have worked camelCase were chosen for naming variables and PascalCase for methods or functions. I came to know later that this is a style many programming languages choose. Infact in langauges like golang if you write snake_cases linter will ask you to correct the name. But for SQL and R I would highly recommend snake_cases as many databases like postgres dont allow capital cases in column names you have to surround names in quotes if you need to use uppercase letters. In R tidyverse has gained huge momentum and now all the packages are following suite. Apart from that if your package can even tell what datatype are you working on that is a huge add on. Packages like stringi and quanteda are excellent example of this. And I would like to add no matter what you choose Please never include dot in any name. Thats a practice valid for only R code and it too is not accepted anywhere apart from R programming language. Overall choose a naming convention for a project and stick to it or ask your client if they have a preference on it. This saves you from trouble of code reviews. 5.2 Informative Names I may sound like a tidyverse fanboy ( I am not) but classes and data types in R are quite opaque so names of functions and objects should reflect precisely what they represent. There is no harm in using names with data-types before them # int_currency &lt;- 1:10 # chr_letters &lt;- letters # dt_mtcars &lt;- data.table::data.table(mtcars) # tbl_mtcars &lt;- tibble::tibble(mtcars) Above advice may be more useful for package developers but it can be used in broad scenarios even on a project where there are multiple working on a same project. If I know what datatype I am dealing with I dont have to go through the entire code and working on top of it becomes that much easier. You can use more descriptive names without data types in the beginning for your projects. Names like data, mainData, dummyVar, tempList etc.. should never be used in a project. Use more descriptive names like sales_data_2020, api_token, rate_of_interest etc 5.3 Conclusions Proper naming conventions will help collaboration in big teams and it makes the code easier to maintain. We should all strive for better names in the code. Its the hardest job to come up with new and unique names for a variable everytime you create one but this is the difference between an average programmer and a good one. Choose a naming convention and stick to it Dont include dots ( . ) in names Use informative names "],["envManagement.html", "Chapter 6 Environment Management 6.1 Avoid package dependencies when possible 6.2 renv for package management 6.3 config for external dependencies 6.4 Conclusion", " Chapter 6 Environment Management If you create a product today be it an API or Shiny App or Even a normal R-script. One thing you cant be sure of is to update the packages or the version of R. There are companies where you can not access different version of a package because multiple projects are relying on the same copy of the package. Its hard to update your package in these companies and you will need to get permissions from top admins to do so. Thus its better to rely on as less packages as possible and that too on the popular ones. But even after you have created a code you would want to keep a record of all the packages and their version as it is for that particular project. This is where environments come in handy. 6.1 Avoid package dependencies when possible Adding one tiny package to your work flow adds recursive dependency not only on the package that you imported but all the other package that your package is relying on and on packages that those packages rely on and so on and on I have also worked on organizations where you have to write an email to explain why you need a certain package to be installed on Rstudio cloud and why you cant get away with already installed packages. I hope you never have to work in such environment ever. But its a better software practice to keep dependencies as minimum as possible. Because each new thing brings a whole set of debugging issues and problems. Mostly this applies to the fact that you can get away with an lapply instead of relying on purrr::map . If your data is very small and you dont do much fancy stuff with it, May be you can get away with base R dataframes instead of tibble or data.table. With new R 4.1.0 you might as well can get away with base pipe |&gt; without magrittr pipe %&gt;%. These are certain examples I can think out of my head. But the implications are huge. If you can achieve something without relying on external dependencies be it a package or anything else you should always choose the one with less dependencies. 6.2 renv for package management There was a package called Packrat a few years ago I would have suggested you to use that always. But currently there is a package I have been using for over a year now by name renv. It does everything that you need to recreate your environment anywhere else. Basically you need to activate the package in your project. By using this command. renv::activate() Then take a snapshot of current project where it will record a list of all the packages used in your project by this command. renv::snapshot() and When you want to reproduce it on a docker container or a remote machine or any place else. You would simple need to run. renv::restore() and it generates a lock file with all the information about a project including the version or R and the versions of the packages used so at any time you can recreate the entire environment again. I could give you multiple ways of tackling the same problem. But this book is about the best possible one so this is it. You just need to use this package to solve almost all of your problems. 6.3 config for external dependencies There is a package called config that allows you to read yaml format in R. That is a standard practice to keep all the Credentials, tokens, API keys etc.. in a config file. There are many other ways you can secure credentials and everything but config is easiest amongst them all and you can use it for storing all the parameters and external path variables that your code requires. It could be an address to external file storage or anything else. Its good to keep all the variable your code requires outside the main code so that when you need to update them you dont need to change the entire code itself. Below is a snippet of config file from one of my project. default: datawarehouse: driver: Postgres server: localhost uid: postgres pwd: postgres port: 5432 database: master dockerdatabase: driver: Postgres server: postgres_plum uid: postgres pwd: postgres port: 5432 database: master filestructure: logfile: &quot;logs/logs.csv&quot; as you can see I havent only kept the passwords and user names but external files as well. Tomorrow if I have to change the logging file I will just have to update it here without opening any R code. It removes so much burden on reading the code again and again. Use it whenever possible. 6.4 Conclusion This chapter doesnt discuss much on concepts but the takeaways from the chapter are: Use as less packages as possible, it helps in code maintenance and debugging. Use renv for all the project you plan to maintain or keep for long term Use config to manage all the external dependencies your project have or might have "],["dataManagement.html", "Chapter 7 data Management 7.1 Keep a Copy or your Data 7.2 Dont use numbers for columns 7.3 Keep Meaningful and proper column names 7.4 Use Databases 7.5 Use Efficient Packages 7.6 Conclusion", " Chapter 7 data Management Because we are either data scientist or data engineers or data analyst or something close to data. R language is specifically tailored towards people working on data so I assume my guess above is correct to some degree. You should also understand a certain principles that go with data management in general. We wont be discussing the details but little tricks to work effectively with data. 7.1 Keep a Copy or your Data Always keep a copy of the original data. While working on the data you can mess up the data to a level where you cant bring back the old version of the data. So Always keep a backup copy of the data in some folder you like. This is the primary reason Excel has an undo button but Access dont. And R has a copy on modify syntax. You will have to explicitly tell R to change the copy of your data otherwise R will never mess up the original copy. Which is excellent for EDA and analysis in general. There is a huge section on copy-on-modify to come so at this moment you just need to know that keep a copy of original data somewhere safe. You will mess up the data big time so its okay to have a backup. 7.2 Dont use numbers for columns In R you can use numbers to refer to columns but just because you can doesnt mean you should. I have seen many people referring to columns of dataframe with numbers. Its not okay to do that because then you loose the context when you read it. mtcars[1, c(1,3:5, 8)] ## mpg disp hp drat vs ## Mazda RX4 21 160 110 3.9 0 Can you tell me precisely what columns am I working on? I have seen people writing this type of code in production and get rejected during code-review. Now compare this code to this one. mtcars[1, c(&quot;mpg&quot;,&quot;disp&quot;,&quot;hp&quot;,&quot;drat&quot;, &quot;vs&quot;)] ## mpg disp hp drat vs ## Mazda RX4 21 160 110 3.9 0 Which one is more readable and more clear to a new user or may be it will help you when after 6 months you will look back at it. Trust me you would not remember the column numbers at all. Its already hard to remember column names though. 7.3 Keep Meaningful and proper column names Most of the data we read is from either an excel file or a poorly designed database thus we see column names having Spaces and dots and all sort of funny things. Remember these rules they will help you even when you are designing a database and trying to name columns for a data base. Assume everything is case sensitive Use only lowercase letters, it will help you when you push it in DB Dont use special character except underscore Please dont use spaces at all There are functions in R like make.names but that too wouldnt help you naming your objects properly. When you have very less time go ahead with make.names but other than that please spend at least 1 to 2 hour naming the data. This small exercise will help you for the entire project. 7.4 Use Databases R has a limitation of RAM while handling data. Even though R has a way of dealing with larger than RAM data I will advice you to use a database instead. I would highly recommend disk.frame package if you are working on larger than RAM data. Its based on data.table and is pretty fast. But that too has limitation and its absolutely not a replacement for a database. There are tons of tools out there from disk.frame to ff to spark and what not but they arent a replacement for a real database. Even files stored on your system arent a proper replacement of a Database. FIrst and foremost requirement of being a data scientist is the ability to work with a database. Sooner or later you will need it. Please store your data in a proper database and learn some basics of database architecture. You dont need to learn everything but a little understanding of normalization will help you a long way in your journey. Use a database in all your project and save the credentials in a config file so that you can change them during production. Even if its just a prototype its okay to start with a database. So that you dont have to change the code once you migrate. The sooner you move your files to database the better it is for you. Using databases will bring advantages like: Early integration of DB in the project Understand the data types of your data as early as possible Call only the amount of data you need thus saving RAM Push computations to DB with packages like modeldb and dbplyr Basic computation like min, max and group by can be done more effectively Never worry about loosing your data or corrupting it There will always be a need for a database in any project what so ever. Please use it as early as possible. 7.5 Use Efficient Packages Working on data is the most memory consuming task. This is where you should understand the scale or your work. If you are developing something for only 5-10 people then you can get away with anything you do. Even though based on your app or api or product people will judge if R is a good language or not or should we be using R again. Than too I would not advice you to think much for such a small use case. Then when you start talking about 100 to 200 users active through out every second. In this case the difference between dplyr vs data.table can actually save the day. There are packages which can help you save memory and speed at the same time. I cant write an exhaustive list of all the packages but I can point you out to the packages and the situations where they will be helpful. 7.5.1 data.table Any list of efficient packages cant be started without this name. Its a package you must learn. We have created apps which were like 3-5 times faster than the python counterpart. The sole reason was we had data.table and they didnt. Despite what people say its easy to learn and easy to use in day to day analysis. It doesnt create a copy of the object and every function of this package is optimized for speed. Its faster than even spark for in memory datasets. Read that statement again if you didnt get how huge it is. I have a book in progress for people trying to learn data.table. This is the link you can read it: https://bookdown.org/content/2746/ 7.5.2 Matrix This package has the ability to create and work with sparse matrices. Sparse matrices save a lot of ram and lot of compute power too When you have any matrix where there are too many 0 or empty values. Sparse matrices are useful in those cases. Learn it and use it if possible. 7.5.3 disk.frame When you have data that is bigger than ram but still good enough that it can be stored on a disk. disk.frame is a best choice in those scenarios. For example you have a 15GB file on a 8 gb laptop than its an excellent candidate. It has its uses. However, I would still recommend to use a DB or spark for that matter but disk.frame has its own usage. 7.5.4 modeldb Its a package that can directly compute linear regression, logistic regression and some other calculations in the database. What it does is that it creates a long sql statement that basically computes the result of some algorithm and then sends it to a DB and gets the result back. Its excellent in scenarios when you have too much data and cant be brought back to R safely. Its better to use computation of SQL rather than bringing it to R. 7.5.5 dbplot Its a package for creating plot directly from a SQL database. It sends the computation directly to a database and gets the results needed to create a plot. Again its useful for circumstances where the data is more than R can handle in RAM. Its an excellent package for EDA and must be used by any serious practitioner. 7.5.6 sparklyr Lastly R has an interface to spark directly from R. Folks at Rstudio has made it so easy to use and install that you would feel you are working on a different programming language. Its definitely worth using when you have truly BIG Data. This is not an exhaustive list of all the packages available in R to handle memory efficiently. R has many more packages that have an ability to handle particular type of data or situation very well. But these are the must haves for anybody working with data. 7.6 Conclusion This chapter focus mainly focuses on good practices of working with data from R. And how to handle them with efficient packages. To go through what we learned so far. keep a copy of your data Dont use numbers for column names keep good column names that will help you in entire project use databases use efficient packages that can properly use computer resources available to you. "],["debug.html", "Chapter 8 Debugging 8.1 Browser() and print() are your friend 8.2 Read the functions 8.3 Version Control System 8.4 Make small commits 8.5 Use curly brackets 8.6 Always use named parameters 8.7 Log the errors 8.8 Dont Use already used names 8.9 Use Simple code 8.10 Conclusion", " Chapter 8 Debugging Debugging is a part of being a programmer. You just cant escape it. Its a huge topic and it takes years of experience to master. To understand the limitations of a language and to understand the errors requires too much study and experience. This is something that you learn through experience. But there are certain thing which you can do to make your life easier while debugging. These are the tips I think everyone should follow for better debugging experience. 8.1 Browser() and print() are your friend To me browser() function feels like a scene from movie matrix where you can stop everything around you and decide what is going wrong in the world. When you want to debug any function or a point where you assume that the error lies here, you should use browser() inside that function or script. Dont be scared of using browser() this function makes you familiar with your code and R itself. Browser is useful only when you know what function to look at. Print is your friend when you want to narrow down the candidates which are causing the error. Printing the objects on the console with help you understand what is happening inside a given function at the moment. Its very useful for interactive web apps where reproducing the error is a little tricky. A combination of print and browser can save a lot of your time. And yes make sure you delete the browser functions from your file. You can use ctrl + shift + f to search the entire project and every file in it for searching anything, including browser function. 8.2 Read the functions R gives you the ability to read the function and it comes very handy during debugging a function you have not written. You can view any function definition by running the function without () round brackets. like this quanteda::tokens ## function (x, what = &quot;word&quot;, remove_punct = FALSE, remove_symbols = FALSE, ## remove_numbers = FALSE, remove_url = FALSE, remove_separators = TRUE, ## split_hyphens = FALSE, include_docvars = TRUE, padding = FALSE, ## verbose = quanteda_options(&quot;verbose&quot;), ...) ## { ## tokens_env$START_TIME &lt;- proc.time() ## object_class &lt;- class(x)[1] ## if (verbose) ## catm(&quot;Creating a tokens object from a&quot;, object_class, ## &quot;input...\\n&quot;) ## UseMethod(&quot;tokens&quot;) ## } ## &lt;bytecode: 0x000000001c924e60&gt; ## &lt;environment: namespace:quanteda&gt; And you can check which methods are available for which classes by using methods(class = &quot;dfm&quot;) ## [1] - ! $ $&lt;- %% ## [6] %*% %/% &amp; * / ## [11] [ [[ [&lt;- ^ + ## [16] all any anyNA Arith as.data.frame ## [21] as.dfm as.logical as.matrix as.numeric bootstrap_dfm ## [26] cbind cbind2 coerce coerce&lt;- colMeans ## [31] colSums Compare convert dfm dfm_compress ## [36] dfm_group dfm_lookup dfm_match dfm_replace dfm_sample ## [41] dfm_select dfm_smooth dfm_sort dfm_subset dfm_tfidf ## [46] dfm_tolower dfm_toupper dfm_trim dfm_weight dfm_wordstem ## [51] dim dim&lt;- dimnames dimnames&lt;- docfreq ## [56] docid docnames docnames&lt;- docvars docvars&lt;- ## [61] fcm featfreq featnames head initialize ## [66] is.finite is.infinite is.na kronecker length ## [71] log Logic Math Math2 meta ## [76] meta&lt;- ndoc nfeat ntoken ntype ## [81] Ops print rbind rbind2 rep ## [86] rowMeans rownames&lt;- rowSums show sparsity ## [91] Summary t tail topfeatures ## see &#39;?methods&#39; for accessing help and source code Or you can check how many classes have a method by same name with. methods(generic.function = &quot;print&quot;)[1:20] ## [1] &quot;print,ANY-method&quot; &quot;print,dfm-method&quot; ## [3] &quot;print,diagonalMatrix-method&quot; &quot;print,dictionary2-method&quot; ## [5] &quot;print,fcm-method&quot; &quot;print,sparseMatrix-method&quot; ## [7] &quot;print.acf&quot; &quot;print.AES&quot; ## [9] &quot;print.all_vars&quot; &quot;print.anova&quot; ## [11] &quot;print.any_vars&quot; &quot;print.aov&quot; ## [13] &quot;print.aovlist&quot; &quot;print.ar&quot; ## [15] &quot;print.Arima&quot; &quot;print.arima0&quot; ## [17] &quot;print.AsIs&quot; &quot;print.aspell&quot; ## [19] &quot;print.aspell_inspect_context&quot; &quot;print.bibentry&quot; now most of these methods are hidden from general usage so you might not be able to view them. # textstat_lexdiv.dfm # will not work will produce an error # Error: object &#39;textstat_lexdiv.dfm&#39; not found quanteda.textstats::textstat_lexdiv ## function (x, measure = c(&quot;TTR&quot;, &quot;C&quot;, &quot;R&quot;, &quot;CTTR&quot;, &quot;U&quot;, &quot;S&quot;, &quot;K&quot;, ## &quot;I&quot;, &quot;D&quot;, &quot;Vm&quot;, &quot;Maas&quot;, &quot;MATTR&quot;, &quot;MSTTR&quot;, &quot;all&quot;), remove_numbers = TRUE, ## remove_punct = TRUE, remove_symbols = TRUE, remove_hyphens = FALSE, ## log.base = 10, MATTR_window = 100L, MSTTR_segment = 100L, ## ...) ## { ## measure &lt;- match.arg(measure, c(&quot;TTR&quot;, &quot;C&quot;, &quot;R&quot;, &quot;CTTR&quot;, ## &quot;U&quot;, &quot;S&quot;, &quot;K&quot;, &quot;I&quot;, &quot;D&quot;, &quot;Vm&quot;, &quot;Maas&quot;, &quot;MATTR&quot;, &quot;MSTTR&quot;, ## &quot;all&quot;), several.ok = TRUE) ## UseMethod(&quot;textstat_lexdiv&quot;) ## } ## &lt;bytecode: 0x0000000019eee9b0&gt; ## &lt;environment: namespace:quanteda.textstats&gt; # works but the implementation is still hidden because method will be decided based on the class provided to the method at the exact moment of calculation But If you still want to know how to know the definition of a method of the class. Just use this code. getAnywhere(&quot;textstat_lexdiv.dfm&quot;) ## A single object matching &#39;textstat_lexdiv.dfm&#39; was found ## It was found in the following places ## namespace:quanteda.textstats ## with value ## ## function (x, measure = c(&quot;TTR&quot;, &quot;C&quot;, &quot;R&quot;, &quot;CTTR&quot;, &quot;U&quot;, &quot;S&quot;, &quot;K&quot;, ## &quot;I&quot;, &quot;D&quot;, &quot;Vm&quot;, &quot;Maas&quot;, &quot;all&quot;), remove_numbers = TRUE, remove_punct = TRUE, ## remove_symbols = TRUE, remove_hyphens = FALSE, log.base = 10, ## ...) ## { ## tokens_only_measures &lt;- c(&quot;MATTR&quot;, &quot;MSTTR&quot;) ## x &lt;- as.dfm(x) ## if (!sum(x)) ## stop(message_error(&quot;dfm_empty&quot;)) ## if (remove_hyphens) ## x &lt;- dfm_split_hyphenated_features(x) ## removals &lt;- removals_regex(separators = FALSE, punct = remove_punct, ## symbols = remove_symbols, numbers = remove_numbers, url = TRUE) ## if (length(removals)) { ## x &lt;- dfm_remove(x, paste(unlist(removals), collapse = &quot;|&quot;), ## valuetype = &quot;regex&quot;) ## } ## if (!sum(x)) ## stop(message_error(&quot;dfm_empty after removal of numbers, symbols, punctuations, hyphens&quot;)) ## if (any(tokens_only_measures %in% measure)) ## stop(&quot;average-based measures are only available for tokens inputs&quot;) ## available_measures &lt;- as.character(formals()$measure)[-1] ## measure &lt;- match.arg(measure, choices = available_measures, ## several.ok = !missing(measure)) ## if (&quot;all&quot; %in% measure) ## measure &lt;- available_measures[!available_measures %in% ## &quot;all&quot;] ## compute_lexdiv_dfm_stats(x, measure = measure, log.base = log.base) ## } ## &lt;bytecode: 0x000000001d67e4d8&gt; ## &lt;environment: namespace:quanteda.textstats&gt; If you want to understand more of this learn OOPS in R. R has multiple object oriented systems and R is a highly Object oriented programming but the style is different from other languages. This book is all about best practices in R and thus we are not going to go deep into fundamentals of R programming here but this trick is worth knowing. These tricks will help you read code that is loaded on your environment but you have not written them. Reading someone elses code makes you a better coder. And it helps you understand why this code is breaking up. 8.3 Version Control System Use a version control system. For those who dont know, it means you can commit changes to a central repository and compare the changes anytime. GitHub and BitBucket are the most popular of these solution. Github allows a free account for every individual. Even for personal projects I would recommend you to use github or any version control system as such. For bigger projects use the one your organization recommends. This will help you compare changes you commit and go back to the old version that is up and running. It sounds easy but the power to compare what you changed in the code can help you pin point the error as quickly as possible. 8.4 Make small commits You should always use small commits. I have seen people who keep the code with themselves for days and change a thousand thing in the code before pushing it to github. I too am one of those people. Make small changes to your code and see if its working and then commit those changes. The smaller the commits the better debugging experience you have. Then its easier to roll back the changes and its easier to read the code to understand what might have caused this error. 8.5 Use curly brackets R gives you the ability to write code without {} but It makes your code harder to read and understand the blocks in segregation. I have seen people write code like this # if statements if( TRUE ) print(TRUE) else print(FALSE) ## [1] TRUE # loops for(i in 1:10) print(i) ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 # functions function(x) print(x) ## function(x) print(x) It sure makes your code look concise but only when its as small as what I wrote. Even then I would advice you to use curly brackets in all possible scenarios. Which helps specially when the code gets bigger or when you are using multiple of these statements together. Lets take this code for example. function(x) for(i in 1:10) if( i %% 2 == 0 ) print(TRUE) else print(FALSE) ## function(x) ## for(i in 1:10) ## if( i %% 2 == 0 ) print(TRUE) else print(FALSE) During big apps you are never sure of how many lines you need to write inside a function or a loop or a conditional and you have to update your code frequently. Without curly brackets it gets harder and harder to pin point the block that is causing the error. So in short using curly brackets help you understand the logic a little better and makes it easier to pin point the block thats causing the error. 8.6 Always use named parameters Lets compare two code in the below chunk. # Code with named parameters # # call_cognitive_endpoint( # endpoint = speech$get_endpoint(), # operation = &quot;models/base&quot;, # body = list(), # options = list(), # headers = list(`content-type` = &#39;audio/wav&#39;), # http_verb = &quot;POST&quot; # ) # ----------------- # Code without named parameters # # call_cognitive_endpoint( # speech$get_endpoint(), # &quot;models/base&quot;, # list(), # list(), # list(`content-type` = &#39;audio/wav&#39;), # &quot;POST&quot; # ) They are commented because we are only focusing on the structure of the function not the working of the function. Which of these codes looks more readable to you? Can you be sure in the below function that you have provided arguments in the right order and you are using the function exactly as it is meant to be used? This is why named parameters actually save time during debugging compared to unnamed one. This also makes code transferable which means that any new person in the team can quickly pick up where you left of, because may be you are so familiar with a function that you assume naming the parameter is not required at all but someone else might not be so much familiar with it. In big organizations where people come and go and anybody can be reassigned to the same code it helps to make it easy to read. And it will you in the long run when you will read your own code after say 1 to 2 years. 8.7 Log the errors This is an old advice I used in functions chapter where I asked you to use tryCatch in all the functions so that it doesnt brake during production. To extend that I would also argue to add logs specially the errors onto a json file or a database table. When you are running your code on your own computer you might get only 1000 bugs and you are prepared to handle those bugs in production. But suppose you create a shiny app that is used by 1000 more people. In those circumstances you will encounter which you might not be able to reproduce so easily and no body will tell you what bug your app still has. Logging errors is a standard practice in programming domain and its necessary for production grade apps, be it shiny app or a REST api. There are many packages available in R for logging I dont have a preference on any of them. Its good to use a database instead of json. 8.8 Dont Use already used names R allows you to override variable and function names that exist. But this is something you shouldnt do. Not even once. I get it when you dont know it collides with something but when you do you should avoid those names at all costs. Take fro example T and F are just variables which have TRUE and FALSE values stored in them. They are not a replacement for boolean values. T &lt;- FALSE myvar &lt;- TRUE if( myvar == T ) print(TRUE) else print(FALSE) ## [1] FALSE Here all your logic is gone because someone thought of renaming A variable T. Most common such error occurs on naming the object remember that these are all valid functions in base R. dt df data These are just a few examples where you can mess up your code very easily without realizing that you are doing something very wrong here. Just like other programming languages will throw an error if you use an already defined name and wont allow you to reuse it, you should treat R the same even though it will not throw an error and you might be lucky enough that it will never throw and error. But you should get into a habit of not reusing function and object names in R as well. 8.9 Use Simple code R gives you a lot of flexibility in coding style. You can write very succinct and precise code with R with highly complex methods. But try to spread your code in decent number of lines so that you can read it later on. Let me give you a very basic example. x &lt;- y &lt;- z &lt;- 1:10 ## or x &lt;- 3; y &lt;- 5; z &lt;- 8 ## or foo &lt;- function(x){ y &lt;&lt;- x } ## &lt;&lt;- is permisable only in very very very rare scenarios This is doable in R but doesnt mean you should do it. This code could easily have been split into 6 lines and it will increase the readability of your code. People from specially maths , finance, science etc background love to write complicated equations and they carry the same attitude to their coding style too However coding is more about code maintenance than about writing code and you cant hope that next person will have equal abilities that you do. Write simple and beautiful code is the best advice I can give in this entire chapter. This makes your life easier and of the people working with you. 8.10 Conclusion In this chapter we discussed multiple strategies of dealing with and avoiding debugging complexities. Hope if you follow most of these tricks you will feel that you are a better debugger than you were before and it will save you a lot of time in the process. To recap what we have learned today. Use print to point of where your code fails use browser to check the code always delete browser functions read the function You can even read hidden methods in R Use version control system make small commits use curly brackets in all your code Pass all arguments to a function through their name and not the position log the errors avoid already used function or object names write simple code avoid using T, F , &lt;&lt;- &amp; ; "],["types.html", "Chapter 9 Type System 9.1 Things you should know 9.2 Choose data types carefully 9.3 dont change datatypes 9.4 Future of type-system in R 9.5 Conclusion", " Chapter 9 Type System With Great power comes great responsibility  ( Uncle Ben ) Man who raised spider-man Despite what most people believe R too has data types. Every language tries to consume the memory space as efficiently as possible and for that they have pre-specified memory layouts that work almost all the same in every language. If you have worked on SQL databases the role of data types are exactly the same across all the languages. R just makes it easier to infer the data-type from your code so that you dont have to declare it specifically. primal data types for vectors in R are : logical numeric integer complex character raw then there are composite data types like date, posixct, even a dataframe is a list with some rules. 9.1 Things you should know There are a certain things you should know about data types in R. 9.1.1 R dont have scalar data types x &lt;- 10L x ## [1] 10 There is a reason [1] is written before the number 10. Its because unlike other languages R dont have scalar values. Everything in R is a vector. It may be a vector of length 1 or 1 billion but its all still a vector. This is one of the primary reason R works more like SQL ( as most data guy love ) and less like JAVA ( as most programmers love ). It gives huge speed to data manipulation as all the operations are more like In Memory Columnar Table. But in return when you are creating a webpage or an API or something where you need a scalar value to be updated again and again. R consumes more resources to do that kind of thing. microbenchmark::microbenchmark( vectorized = { x &lt;- rnorm(1e3) }, scalar = { y &lt;- numeric(1L) for(i in 1:1e3){ y[[i]] &lt;- rnorm(1) } }, times = 10 ) ## Unit: microseconds ## expr min lq mean median uq max neval cld ## vectorized 69.3 83.9 105.93 89.4 112.2 207.3 10 a ## scalar 4951.6 5466.1 10923.62 6258.0 6848.6 54768.5 10 b Even then I would ask you to go ahead with R because the difference will most probably in 1-2 milliseconds which will never impact your performance any serious way. But this is something you should remember that vectorized R is way faster than even python but scalar manipulations in R are a bit slower. Choose vectorized version of the code whenever possible even if you do a bit more steps in it, it will still be faster than the scalar versions. 9.1.2 Dates are basically integers under the hood. x &lt;- Sys.Date() class(x) ## [1] &quot;Date&quot; # &quot;Date&quot; as.integer(x) ## [1] 18726 This number means it has been 18 thousand 7 hundred days since 1970 which is roughly (365 * 50) 9.1.3 POSIXlt are basically lists under the hood unclass(x) ## [1] 18726 y &lt;- Sys.time() y &lt;- as.POSIXlt(y) class(y) ## [1] &quot;POSIXlt&quot; &quot;POSIXt&quot; # &quot;POSIXlt&quot; unclass(y) ## $sec ## [1] 53.06889 ## ## $min ## [1] 29 ## ## $hour ## [1] 10 ## ## $mday ## [1] 9 ## ## $mon ## [1] 3 ## ## $year ## [1] 121 ## ## $wday ## [1] 5 ## ## $yday ## [1] 98 ## ## $isdst ## [1] 0 ## ## $zone ## [1] &quot;IST&quot; ## ## $gmtoff ## [1] 19800 ## ## attr(,&quot;tzone&quot;) ## [1] &quot;&quot; &quot;IST&quot; &quot;+0630&quot; because it stores metadata along with it, use posixct whenever possible. 9.1.4 Integers are smaller than numeric x &lt;- sample(1:1e3, 1e8, replace = TRUE) class(x) ## [1] &quot;integer&quot; # [1] &quot;integer&quot; y &lt;- as.numeric(x) class(y) ## [1] &quot;numeric&quot; # [1] &quot;numeric&quot; object.size(x) ## 400000048 bytes # 400000048 bytes object.size(y) ## 800000048 bytes # 800000048 bytes See the difference yourself. Its about twice the size of the original integer vector. Its all because of datatypes. You should use integer only when you need one. There is a cool trick to let R know that you are creating an integer. Just add L at the end. x &lt;- 1 class(x) ## [1] &quot;numeric&quot; #[1] &quot;numeric&quot; y &lt;- 1L class(y) ## [1] &quot;integer&quot; # [1] &quot;integer&quot; Letter L at the end after a number will tell R that you want an integer. Please use integers when you need one. 9.1.5 define your datatypes before the variable i &lt;- integer(1e3) class(i) ## [1] &quot;integer&quot; length(i) ## [1] 1000 l &lt;- logical(1e3) class(l) ## [1] &quot;logical&quot; length(l) ## [1] 1000 n &lt;- numeric(1e3) class(n) ## [1] &quot;numeric&quot; length(n) ## [1] 1000 c &lt;- character(1e3) class(c) ## [1] &quot;character&quot; length(c) ## [1] 1000 Just like any other language even in R you can create an empty vector of a predefined length which are initialized at 0 or \"\" or FALSE based on the data types. Use this functionality when you want to create a column or vector you know nothing about except data type. Defining data-types beforehand is an excellent programming practice and we as R user should use it more often. It also removes burden on the compiler to try to guess the data-type. 9.1.6 lists are better than dataframe under a loop # x_dataframe &lt;- data.frame(x = 1:1e3L) # # for(i in 1:1e4L){ # x_dataframe$x[[i]] &lt;- i # } # This code will produce an error because you can&#39;t increase the row count of a dataframe like that. x_list &lt;- list(x = 1:1e3L) for(i in 1:1e4L){ x_list$x[[i]] &lt;- i } x_dataframe &lt;- as.data.frame(x_list) class(x_dataframe) ## [1] &quot;data.frame&quot; you can not create additional rows easily in data.frame. but all dataframes are lists under the hood with some additional rules. you can convert them to list run a loop and convert back to data.frame. Its not only efficient but its faster too 9.1.7 use lists whenever possible Other languages have structs to handle multiple object types. R have lists and lists are most versatile piece of data-type you will find across any language. There are tons of example like the one I provided above where lists are more efficient because they dont have any restrictions. In my personal use case I have seen people trying to put a square peg in round hole by using data.frames at places where a simple list will be more efficient and appropriate. Please use list as frequently as possible and remember, always opt for lower level data type for better memory management. 9.2 Choose data types carefully As we saw in examples above choosing the right data-type can mean a lot in x &lt;- seq.Date( from = as.Date(&quot;2000-01-01&quot;), to = Sys.Date(), length.out = 1e4) x &lt;- sample(x, 1e8, TRUE) y &lt;- data.table::as.IDate(x) length(x) == length(y) ## [1] TRUE object.size(x) ## 800000272 bytes object.size(y) ## 400000336 bytes as you can see the base date type consumes around twice the memory compared to IDate data type from data.table. It may be because one is using numeric data types under the hood and other is using integers under the hood and it makes a huge difference when you are working on big data, to understand the data types in R and properly map them to save more space on your RAM. Despite what most people say RAM and CPU are not cheap, throwing more processor on something should only be done when the code is properly optimized. I dont want you to be hyper optimize your code on every sentence you write but being aware of some best practices will surely help you along the way. Go to next section for speed optimization as well. There are many packages in R we will talk about that provide speed ups to the code and saves memory too We will talk about them later in the book. At this point all I can say is if R is slow or it crashes may be the data-type you have chosen is not right fit for the job. Try changing it and it will work just fine. 9.3 dont change datatypes R gives you an option to do this. x &lt;- &quot;Hello world&quot; print(x) ## [1] &quot;Hello world&quot; x &lt;- 1L print(x) ## [1] 1 Now you just assigned x as a character vector and then replaced x as an integer vector. This is something you can do but its something you should never do. changing the datatype of a vector is not recommended in any programming language unless you are trying to convert from one data-type to another. like : x &lt;- &quot;2020-01-01&quot; class(x) ## [1] &quot;character&quot; x &lt;- as.Date(x) class(x) ## [1] &quot;Date&quot; y &lt;- c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;) class(y) ## [1] &quot;character&quot; y &lt;- as.integer(y) class(y) ## [1] &quot;integer&quot; This and many operations like this where you know beforehand that you need to change the data-type is a good example of cases where you must change the data-types. So apart from cases where data-conversion is needed you should never change the data-types ever. Its a bad practice to do so. This is one of the scenario when you have the power but you mustnt use it. 9.4 Future of type-system in R Type system is important when you really want to save memory. Its specially true when you are dealing with huge volume of data and you want to save RAM more efficiently as possible, which is what an R users bread and butter. Its more like when you need it you really cant do without it. Every programming language is understanding this now. R is no exception to the rule. people are coming up with excellent theories on how to integrate a type system in R. Sooner or Later we will have a proper type system. Currently, There is a package called Typed by Antoine Fabri on CRAN. You can install it directly from cran. It will not give you speed benefits though because it doesnt talk to compiler directly but it surely will restrict people to using wrong data-types where you dont need them. Its helpful when you write functions where you only need vector of certain lengths and a certain type so that further operations can be optimized. Then there are packages like contractr by By Alexi Turcotte, Aviral Goel, Filip Kikava, Jan Vitek which can talk to compilers directly. Package is still in early development have no claims or documents available at their homepage at https://prl-prg.github.io/contractr/. But they are trying to insert type system through roxygen arguements above a function. I think this will be useful for package developers. It has a long way to go but we are thinking in right direction at least. For more information you should watch this video These packages are no substitute for an inbuilt type system and we may ignore it but we really need a type system going forward. Lets hope for the best and have our fingers crossed for the moment. 9.5 Conclusion In This chapter we focused on multiple data-types in R and how to save memory and CPU time by utilizing the best one in it. There are only a few but critical takeaways from this chapter : remember: R dont have scalars dates are integers POSIXlt should rarely be used use integers when you can define data-types beforehand use lists wherever possible choose data-types carefully Dont change data-types unless necessary In Future we will have a type-system and we should learn to love types early on. "],["reference.html", "Chapter 10 Pass By Value-Reference 10.1 Understanding the system 10.2 Copy on modify 10.3 for pass by reference 10.4 Conclusion", " Chapter 10 Pass By Value-Reference In programming we have a concept of how to pass a value to a function. If we can do away with modification of the object inside the function then its okay to pass the original object and let it change else we can create a copy of it and let the function modify it at will without effecting the object itself. understanding this concept is very crucial if you want to write efficient code. Lets dive deeper into it. 10.1 Understanding the system There are mostly 2 systems available for passing the objects from one function to another. Lets understand both of them. 10.1.1 Pass by Value This is when you create a copy of the original object and pass it to the function. Because you are actually passing just a copy to the function whatever you do to the object doesnt impact the original one. Lets check it by an example. x &lt;- list(y = 1:10) pass_by_value &lt;- function(x){ x$y &lt;- 10:1 } pass_by_value(x) x$y ## [1] 1 2 3 4 5 6 7 8 9 10 now x was passed to the function and modified yet it remains same because only copy of the object was passed to the function ( Well, not precisely but this is what we will discuss later). 10.1.2 Pass by reference This is when you pass the entire object as is. Basically you pass the pointer to the original object and now if you change the object you change the original copy of it. Lets check the same example again. x &lt;- new.env() x$y &lt;- 1:10 pass_by_value &lt;- function(x){ x$y &lt;- 10:1 } pass_by_value(x) x$y ## [1] 10 9 8 7 6 5 4 3 2 1 Now x was passed by reference and no copy was assigned to the function. So when you changed the object inside the function original object was changed. Hope you now understand practically what does the word mean. 10.2 Copy on modify R has no effective means to specify when to pass with value and when to pass with reference. And because there are only 2 ways to deal with this problem everybody assumes that R does create a copy of the object every time it passes the object through a function. But R has a different way of doing things which is called copy of modify. There are better blogs written over it and nuances are very peculiar which while writing code you shouldnt worry about much. I will try to simplify the concept from the practical point of you view so that you can use it in real life without much thought to it. R basically passes an object by references until you modify it. Lets check it live: mt_tbl &lt;- data.frame(mtcars) tracemem(mt_tbl) ## [1] &quot;&lt;0000000021946E38&gt;&quot; dummy_tbl &lt;- mt_tbl ## No tracemem yet mpg_col &lt;- as.character(mt_tbl$mpg) ## No tracemem yet mt_tbl[ (mt_tbl$cyl == 6) &amp; (mt_tbl$hp &gt; 90), ] ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## No tracemem yet subset( mt_tbl, cyl == 6) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## No tracemem yet tracemem is a function that will return memory address every time the object is copied. So far it didnt return anything even though it passed through so many functions and each of those functions must be using multiple functions internally. Yet no copy of the object was made. Because So Far we havent modified anything. now look at the code below. mt_tbl %&gt;% filter(cyl == 6, hp &gt; 90) %&gt;% group_by(gear) %&gt;% summarise(n()) %&gt;% select(gear) ## tracemem[0x0000000021946e38 -&gt; 0x000000001ef53608]: initialize &lt;Anonymous&gt; filter_rows filter.data.frame filter group_by summarise select %&gt;% eval eval withVisible withCallingHandlers handle timing_fn evaluate_call &lt;Anonymous&gt; evaluate in_dir block_exec call_block process_group.block process_group withCallingHandlers process_file &lt;Anonymous&gt; &lt;Anonymous&gt; render_cur_session render_book render_book_script in_dir &lt;Anonymous&gt; &lt;Anonymous&gt; ## tracemem[0x000000001ef53608 -&gt; 0x000000001ef536b8]: initialize &lt;Anonymous&gt; filter_rows filter.data.frame filter group_by summarise select %&gt;% eval eval withVisible withCallingHandlers handle timing_fn evaluate_call &lt;Anonymous&gt; evaluate in_dir block_exec call_block process_group.block process_group withCallingHandlers process_file &lt;Anonymous&gt; &lt;Anonymous&gt; render_cur_session render_book render_book_script in_dir &lt;Anonymous&gt; &lt;Anonymous&gt; ## # A tibble: 3 x 1 ## gear ## &lt;dbl&gt; ## 1 3 ## 2 4 ## 3 5 dplyr will change the data.frame to tibble and trigger tracemem This is one of the reason I absolutely love and recommend data.table to everybody. Which manages memory very efficiently its at par with any in memory table of a DB. If you are actually concerned about memory use data.table. new_tbl &lt;- mt_tbl %&gt;% filter(cyl == 6, hp &gt; 90) %&gt;% group_by(gear) %&gt;% summarise(n()) %&gt;% select(gear) ## tracemem[0x0000000021946e38 -&gt; 0x0000000021684d00]: initialize &lt;Anonymous&gt; filter_rows filter.data.frame filter group_by summarise select %&gt;% eval eval withVisible withCallingHandlers handle timing_fn evaluate_call &lt;Anonymous&gt; evaluate in_dir block_exec call_block process_group.block process_group withCallingHandlers process_file &lt;Anonymous&gt; &lt;Anonymous&gt; render_cur_session render_book render_book_script in_dir &lt;Anonymous&gt; &lt;Anonymous&gt; ## tracemem[0x0000000021684d00 -&gt; 0x0000000021515478]: initialize &lt;Anonymous&gt; filter_rows filter.data.frame filter group_by summarise select %&gt;% eval eval withVisible withCallingHandlers handle timing_fn evaluate_call &lt;Anonymous&gt; evaluate in_dir block_exec call_block process_group.block process_group withCallingHandlers process_file &lt;Anonymous&gt; &lt;Anonymous&gt; render_cur_session render_book render_book_script in_dir &lt;Anonymous&gt; &lt;Anonymous&gt; now we are modifying the results somewhere and thus a copy is created. The actual rules are very very complicated. But in simple term as long as you dont modify any thing R doesnt create a copy and everything is passed down by reference. It impacts speed too Let check it by an example foo &lt;- function(x){ sum(x) } bar &lt;- function(x){ x[[1]] &lt;- 1 sum(x) } As you can see both the functions are identical the only difference is that in bar I am modifying the object while in foo I am not changing the object. Lets run a speed test x &lt;- rnorm(1e7) microbenchmark::microbenchmark( foo = foo(x), bar = bar(x), times = 10 ) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## foo 12.1361 12.4412 12.88698 12.6253 13.3272 13.9405 10 a ## bar 44.0299 44.8220 46.46297 45.4883 47.3347 51.7525 10 b As you can see the difference in time is because bar is creating a copy of the object. And you may assume that it will create a copy at every time you change a object and you will be dead wrong as R is smart enough to understand that It can get away with only single copy of the object. Lets create a function that changes more things in x and see the difference. bar_new &lt;- function(x){ x[[1]] &lt;- 1 x[[10]] &lt;- 10 x[[1e3]] &lt;- 1e3 sum(x) } microbenchmark::microbenchmark( foo = foo(x), bar = bar(x), bar_new = bar_new(x), times = 10 ) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## foo 12.3144 12.5421 13.24573 13.28840 13.7758 14.7688 10 a ## bar 45.4869 47.1599 62.16034 51.13310 53.1140 168.9597 10 b ## bar_new 44.0454 44.4826 49.03338 48.81365 50.8288 63.2162 10 b Now as you can see that while the function foo and bar have significant differences in performance, same is not true for bar and bar_new. Because bar_new too creates a copy but maintains that copy for the entire function. So R is smart enough to understand when to create a copy and when not to create a copy. Once a copy is created it is retained in R and R uses it smartly. We can gain speed and memory benefits by making sure all the modification is done inside a single function. So that R doesnt create much copies. Instead of using bar 3 times its better to use bar_new once. So that you dont copy it multiple times. See the difference for yourself. And thus try to keep all the modifications close and in as less functions as possible. microbenchmark::microbenchmark( bar = { bar(x) bar(x) bar(x) }, bar_new = bar_new(x), times = 10 ) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## bar 135.1512 138.8276 161.72114 143.94805 144.5945 248.9160 10 b ## bar_new 42.9163 44.5204 57.36501 46.08275 49.5533 157.2224 10 a best is to group these modifications together. So the gist of the matter is: R passes everything by reference until you modify it R creates a copy when you modify the object You should always keep all the Object modifications in same function 10.3 for pass by reference As I told you before R has no way of specifying when the object will be pass by reference and when it will be passed by value. And there are certainly times you wish you had passed it by value and certainly times when you wish you passed it by reference. When you modify something inside a function you create a copy of it. So take example of a loop inside and outside a function x &lt;- numeric(10) for(i in 1:10){ x[[i]] &lt;- rnorm(1) } x ## [1] -0.7600836 -0.4115738 0.7214233 0.6532786 0.1585052 0.4918416 ## [7] -2.1806293 0.1029496 -1.6149167 -0.6524864 It modifies the object in place. Now lets wrap it in a function and see what happens. x &lt;- numeric(10) foo &lt;- function(x){ for(i in 1:10){ x[[i]] &lt;- rnorm(1) } return(x) } foo(x) ## [1] 2.24775725 1.47478992 1.26412149 -0.39399240 -1.88564464 0.08829741 ## [7] 0.38126076 0.42783596 -0.01942583 1.67778634 x ## [1] 0 0 0 0 0 0 0 0 0 0 Now x is not modified because it is being modified inside a function. This is crucial at times when you are running a long job that might take hours to complete just to find an error in the middle. You might want to start the loop from the exact position you left off. With this sort of code you will not reach that. Lets generate an error in the code and uses bigger number. total_length &lt;- 1e2 set.seed(1) x &lt;- numeric(total_length) foo &lt;- function(number){ y &lt;- sample(1:total_length,1) for(i in 1:total_length){ number[[i]] &lt;- i if(y == i){ stop(sprintf(&quot;there is an error at %s&quot;, y)) } } return(number) } foo(x) ## You will get an Error ## Error in foo(x): there is an error at 68 If you run this code you will get an error at some number and x will still be the same. All the processing of code till that moment is lost for everybody. Which is not what you want if each iteration took just 2 minutes to run. This difference could mean hours in some scenarios. R has 4 datatypes that provide mutable objects or pass by reference semantics. R6 Classes environments data.table listenv I wouldnt recommend writing an R6 class just to run a simple loop, however if your use case is pretty complex R6 would be a valid solution for it. We already saw how environments can be used for pass by reference. But passing around environments is not a good idea it requires you to know too much about the language and be very careful with what you are doing hence I only prefer 2 approaches. One with data.table and other with listenv package. But their usecase is very different. One should be used where you are comfortable with lists are more suited while other should be used where data.frame or vectors are more suited for the task. Doing it for listenv is very easy. Its the same code with just the new listenv object. foo_list &lt;- function(list){ y &lt;- sample(1:total_length,1) for(i in 1:total_length){ list$x[[i]] &lt;- i if(y == i){ stop(sprintf(&quot;there is an error at %s&quot;, y)) } } } list_env &lt;- listenv::listenv() list_env$x &lt;- numeric(total_length) foo_list(list_env) ## Error in foo_list(list_env): there is an error at 39 Now again we got an errors but this time all the other changes have been saved in x. list_env$x ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 0 0 0 0 0 0 0 0 0 0 0 ## [51] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [76] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Same thing could be done in data.table as well. lets write a new function for doing it. data.table has 2 ways of looping through the vectors. With := operator which is slow but useful for more data insertion than one by one with set function which is faster where you need to insert data one by one. Lets use the second approach to write a function. x_dt &lt;- data.table::data.table(x = numeric(total_length)) foo_dt &lt;- function(dt){ y &lt;- sample(1:total_length,1) for(i in 1:total_length){ data.table::set( x = dt, i = i, j = &quot;x&quot;, value = i) if(y == i){ stop(sprintf(&quot;there is an error at %s&quot;, y)) } } } foo_dt(x_dt) ## Error in foo_dt(x_dt): there is an error at 1 Now just like again even though we got errors we can still check the ones that have been completed during the loop. x_dt$x ## [1] 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 So let me make things simpler. When you want to modify objects in place you need to use 1 of the 2 approach. When you are working on data.frames and vectors use data.table while when you are working on anything else, anything in general, use listenv approach. 10.4 Conclusion This chapter focused on how to save memory of your R program by using objects through reference and avoid creating copies of the object. Lets summarize what we have read so far. keep all the modifications of objects in a single function use pass by reference through listenv and data.table for saving memory avoid creating multiple copies of an object at all costs "],["releasememory.html", "Chapter 11 Release Memory 11.1 use rm() 11.2 use gc() 11.3 Conclusion", " Chapter 11 Release Memory Sometimes even after applying all the best techniques to save memory you still need more space. In those cases you need to understand that R has 2 techniques available to you. These techniques are mostly useful in different context. I mostly recommend using gc() inside a loop and rm() inside an ETL or R script. 11.1 use rm() rm function deletes any object from R and releases some memory that could be used for further operations. Its excellent for cases where you have a lot of intermediate results saved inside an R script that is supposed to run through an ETL. like this value &lt;- length(mtcars$mpg) sorted_mpg &lt;- sort(mtcars$mpg) median_values &lt;- sorted_mpg[value/2] + sorted_mpg[(value/2) + 1] median_mpg &lt;- median_values/2 rm(list = c(&quot;value&quot;, &quot;sorted_mpg&quot;, &quot;median_values&quot;)) In these scenarios where you are running scripts like these and storing intermediate result for further processing its okay to use rm function. However when you wrap it inside a function only the final result stays and everything else is lost anyways so you dont need to use rm function at all. I would not advice anyone to use this function unless you are writing an ETL script on a data that is large enough that you would need space after every iteration. Even in those scenarios you can get away with not using intermediate variables for storing the results. But if you must and only and only in those very rare cases this function should be used. In broader cases Rs garbage collection will have you covered. 11.2 use gc() Garbage collection is a process by which a software or a program returns the memory back to OS or frees it for its own usage. R has significantly improved its memory management in recent years. All the blogs you read about R being slow and memory hungry are mostly written during 2014-2017. They arent true anymore. 11.2.1 R version 3.5 In R version 3.5 altrep was introduced which helps in saving a lot of memory in special cases like loops. Packages like vroom have achieved significant speed ups because now they didnt have to load all the data in memory and can only load as much as they need. If you want to read more about it you should visit this page https://svn.r-project.org/R/branches/ALTREP/ALTREP.html. 11.2.2 R version 4.0 In R version 4.0 ARC ( automatic reference counting )was introduced in R. Previously R used to keep a track of how many copies an object has with NRC ( named reference counting )and that had only 3 option, 0, 1 and 2; where I think 2 meant many. Now with ARC R can increase and decrease these numbers for keeping a better track of how many object points to the same memory address. You can read more about in a blog post here: https://msmith.de/2020/05/14/exploring-r-reference-counting.html#:~:text=The%20CRAN%20NEWS%20accompanying%20R,further%20optimizations%20in%20the%20future.. So the gist of the matter is that R has been improving performance and memory management for a very long time. Now coming back to the main point. R has a function by name gc to run a garbage collector and release the memory that is not needed anymore. Its useful to run it in mostly 2 situations. 11.2.3 Inside a heavy loop Loops are slow in R and sometime when are you doing heavy manipulation in a loop. R might consume more memory than its needed and will slow the further processes. This is the point I use a gc() inside a loop. gc_after_a_number &lt;- 1e2 for(i in 1:1e4){ ### heavy calculations if( i %% gc_after_a_number == 0){ gc() } } Here in the code above I am running a long loop but after every 100 iterations I am freeing up memory by running a gc() function. After how many iterations do you want to use a loop is totally dependent on what are you doing inside a loop. It could go from anywhere between 10 to 10 thousand depending upon the situation. It surely helps and I would recommend everyone to use it but only for very heavy calculations. 11.2.4 anything that takes more than 30 seconds If you know a function takes more than 30 second to execute. I would suggest you to run a gc() after it. It helps keep a check on Rs memory. Its useful even if you are running a shiny app or an API. If anything takes more than 30 second its worth running the gc() function because than adding a few milliseconds will not be your main problem for sure. 11.3 Conclusion In this chapter we learned how to free up memory for more usage in R. This should help you manage your R environment better. Lets see what we have studied so far. Dont use rm unless necessary rm should only be used in an ETL or a manual script Most of the time you can avoid rm use gc() inside heavy loops after every few iteration use gc() after every heavy transaction which takes more than a couple of seconds. "],["loops.html", "Chapter 12 For Loops 12.1 data-types 12.2 apply family 12.3 Vectorize", " Chapter 12 For Loops This is a topic that I wanted to discuss for a long time. People read blogs from 2014-2016 and assume that for loops in R are bad. You should not use 12.1 data-types 12.1.1 [[ are faster than [ set_df_col &lt;- function(dt){ for(i in 1:1e4){ dt$x[[i]] &lt;- i*2 } } set_dt_num &lt;- function(dt){ for(i in 1:1e4){ data.table::set( x = dt, i = i, j = 1, value = i*2) } } set_dt_col &lt;- function(dt){ for(i in 1:1e4){ data.table::set( x = dt, i = i, j = &quot;x&quot;, value = i*2) } } 12.2 apply family 12.3 Vectorize "],["speedtips.html", "Chapter 13 Some Tips to make R code faster 13.1 Use Latest version of R 13.2 Benchmark the findings 13.3 Algorithm matters more than language 13.4 Read the function 13.5 Use simple functions", " Chapter 13 Some Tips to make R code faster Make it work, then make it beautiful, then if you really, really have to, make it fast. 90 percent of the time, if you make it beautiful, it will already be fast. So really, just make it beautiful!  Joe Armstrong In IT sector speed is very important. People rewrite tons of algorithms back again in c, c++, go and java. Just to gain that miliseconds or may be microseconds performance over one another. When you compared to these languages R is a very slow language. If you really need to get nanosecond level of optimizations in R that are not possible without going to Rcpp; which by the way is a very easy wrapper for R user around C++. But still R code can be optimized to a level where you can get production level efficiency in R without too much trouble. And R is not slow compared to interpreted languages like python, Javascript, ruby etc 13.1 Use Latest version of R Each iteration of R is improving something to gain more speed with less and less memory. Its always useful if you need more speed to switch to Latest version of R and see if you get any speed gains. In general if you are using old versions of R or old functions that are deprecated and are no longer recommended by switching to a new version or new methods you will get a speed advantage for sure. Constant criticism that R is slow has made R to work in this respect and R is evolving according to the needs of time. There is not much to add here. If possible use the latest version, packages or methods mostly they might have more speed. 13.2 Benchmark the findings R is very obscure language there are no direct rules for speed gains. You might think you are making the code fast but in turn you could make it slow. The worst part about R is that you can write very very very slow code in R without realizing what are you missing. same R code can run 1K times faster when optimized. R is a very easy language to write slow code in. This is something you should keep in mind while writing the code. This is the reason you should benchmark your options, It may not give you much speed improvement, it may not give you any speed improvement at all. If you want to optimize R you must learn to benchmark the options. I would not go in details but microbenchmark is the best package for this task. Other packages have too many assumptions. 13.3 Algorithm matters more than language Lets understand this by a very simple example. Lets start with the worst way you could code in any language called recursive functions and mark my words Never Use Recursive Functions. You are always better off without them. Lets try to see if we can find the good old fibonacci numbers and first 20 of them. recurse_fib_r &lt;- function(fnum){ if(fnum &lt;= 1) { return(fnum) } else { return( recurse_fib_r(fnum-1) + recurse_fib_r(fnum-2) ) } } #include &lt;Rcpp.h&gt; //[[Rcpp::export]] int recurse_fib_rcpp(int fnum){ if(fnum &lt;= 1) { return(fnum) ; } else { return recurse_fib_rcpp( fnum - 1 ) + recurse_fib_rcpp( fnum - 2 ) ; } } lets compare both the functions now. microbenchmark::microbenchmark( mapply( recurse_fib_rcpp, 1:30 ), mapply( recurse_fib_r, 1:30 ), times = 10 ) ## Unit: milliseconds ## expr min lq mean median ## mapply(recurse_fib_rcpp, 1:30) 10.3127 10.562 12.84923 13.38575 ## mapply(recurse_fib_r, 1:30) 4838.6412 4911.283 5365.45518 5435.20670 ## uq max neval cld ## 13.5092 18.9751 10 a ## 5446.5246 6165.9305 10 b Lets try to save computation by saving the results mem_fib_r &lt;- function(fnum){ if(fnum &lt;= 1) { return(fnum) } else { return( memoised_fib_r(fnum - 1) + memoised_fib_r( fnum - 2) ) } } memoised_fib_r &lt;- memoise::memoise(mem_fib_r) microbenchmark::microbenchmark( mapply( recurse_fib_rcpp, 1:30 ), mapply( memoised_fib_r, 1:30 ), times = 10 ) ## Unit: milliseconds ## expr min lq mean median uq ## mapply(recurse_fib_rcpp, 1:30) 10.2122 10.887 11.59755 11.47625 12.2490 ## mapply(memoised_fib_r, 1:30) 2.4867 2.902 7.83596 3.11120 3.6502 ## max neval cld ## 13.9800 10 a ## 50.1744 10 a We can still write a better algorithm by writing a loop save_fib_r &lt;- function(fnum){ fnum &lt;- fnum + 1 vec &lt;- integer(fnum) vec[[2]] &lt;- 1 if(fnum &gt; 2){ for(i in 3:fnum){ vec[[i]] &lt;- vec[[ i - 1]] + vec[[ i - 2]] } } return(vec[[fnum]]) } Lets compare the results microbenchmark::microbenchmark( mapply( recurse_fib_rcpp, 1:30 ), mapply( save_fib_r, 1:30 ), times = 10 ) ## Unit: microseconds ## expr min lq mean median uq ## mapply(recurse_fib_rcpp, 1:30) 11279.6 11365.6 12951.81 12016.2 13664.0 ## mapply(save_fib_r, 1:30) 197.4 269.5 1202.90 313.8 364.4 ## max neval cld ## 18894.0 10 b ## 9326.8 10 a Because other functions is vectorized but I am only asking for a single number I am doing the same calculations multiple time inside mapply function. save_vec_fib_r &lt;- function(fnum){ fnum &lt;- fnum + 1 vec &lt;- integer(fnum) vec[[2]] &lt;- 1 if(fnum &gt; 2){ for(i in 3:fnum){ vec[[i]] &lt;- vec[[ i - 1]] + vec[[ i - 2]] } } return(vec) } Now lets compare the differences microbenchmark::microbenchmark( mapply( save_fib_r, 1:1e3 ), save_vec_fib_r(1e3), times = 10 ) ## Unit: microseconds ## expr min lq mean median uq max ## mapply(save_fib_r, 1:1000) 89555.9 92689.6 100563.30 96297.3 103632.1 124244.9 ## save_vec_fib_r(1000) 136.8 141.2 986.89 171.7 276.6 8166.2 ## neval cld ## 10 b ## 10 a 13.4 Read the function 13.5 Use simple functions "],["multithreading.html", "Chapter 14 Multithreading", " Chapter 14 Multithreading "],["packages.html", "Chapter 15 packages", " Chapter 15 packages "],["shinyspeed.html", "Chapter 16 Speed", " Chapter 16 Speed "],["shinymemory.html", "Chapter 17 Memory", " Chapter 17 Memory "]]
