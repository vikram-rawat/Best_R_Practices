[["index.html", "Best Coding Practices for R CoverPage", " Best Coding Practices for R Vikram Singh Rawat 2021-04-23 CoverPage Know the rules well, so you can break them effectively.  The Dalai Lama "],["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction The most damaging phrase in the language is: Its always been done that way.*.  Grace Hopper Did you try to read the title from the cover of the book? I could have done a thousand things to make it easier for you to read it. But this cover reminds me of how often we overlook simple things which are very crucial from the readers point of view. R is an excellent programming language its turing complete and doesnt lack anything for a production level code. It can be used in the entire data domain from APIs to dashboards to apps and much more. Trust the language and trust in yourself. Its a journey everyone has gone through and everyone must go through. R programmers have a bad reputation for not writing production level code. It stems from the fact that we mostly arent trained programmers. We tend to overlook things that are crucial from a programming standpoint. As R programmers we are often less inclined to write the code for production. Mostly we try to write scripts and when we are asked to deploy the same we just wrap it in a function and provide it to the IT team. I have been at the receiving end of these issues where I had to maintain a poorly written code; columns were referred by numbers, functions were dependent upon global environment variables, 50+ lines functions without arguement, poor naming conventions etc. I too am a self taught programmer and have gone through these hiccups of code deployment, code reviews and speed issues. World going forward will all be code and data. The sooner you learn these skills the better it is for you to have trust in your own programming skills. R is a huge language and I would like to share the little knowledge I have in the subject. I dont claim to be an expert but this book will guide you in the right path wherever possible. Most of the books about R programming language will tell you what are the possible ways to do one thing in R. This book will only tell you one way to do that thing correctly. I will try to write it as a dictionary as succinctly as possible. So that you can use it for references. Let the journey begin "],["folder.html", "Chapter 2 Folder Structure 2.1 Organizing files 2.2 Create Projects 2.3 Naming files 2.4 Folders Based on File-Type 2.5 Creating Sub-folders 2.6 Conclusion", " Chapter 2 Folder Structure 2.1 Organizing files The best way to organize your code is to write a package. Organizing your code is the first and foremost thing you should learn. Because as the project grows and multiple files are put into a folder it gets harder to navigate the code. A proper folder structure definitely helps in these times. I Just couldnt emphasis it enough that best way to organize your code is to write a package. But even when you are not planning to write a package. There are best practices to make it readable and make a smooth navigation. 2.2 Create Projects Its such a minor thing to say but I still till date see code like this: setwd(&quot;c://myproject_name/&quot;) It was a good practice like 5 - 6 years ago. Now Rstudio has a feature to create project. new project Once you create a project it is easier to manage your files and folders and its easier to give it somebody as well. It has virtually the same effect but then you can use Rstudio a little better. Its something I recommend to every user regardless of the skill level. 2.3 Naming files I data science most common problem is that we dont change the file names of excel or csv files provided by business people. And most of the time those file names are totally abbreviated with spaces in between and multiple cases like Total Sales Mike 202002-AZ1P2R.csv. This name is useful for the MIS or Business Analyst as they have a different way of organizing files then yours. They might do it because they have to keep a record of different people and have to provide it anytime asked. But as a Data Scientist your work is entirely different. You are not delivering files you are writing code. Let me reiterate this fact YOU ARE WRITING CODE. In most of the scenarios Data Science is more like programming less like science. Even though it has proportion of both of them. Using fundamentals of programming practices will help you out in long term. So change such file names to sales_data_mike_feb2020.csv or something similar. There are no right or wrong names just what makes more sense to a new user. There is a trick about naming conventions: use all lower case or upper case ( helps you in never forgetting the cases ) use underscore in between ( Because file names are mostly long Camel Or Pascal cases may confuse users) make the name as general as possible ( make sure a newcomer should be able to understand it without any problem) In choosing a name there are no wrong answers only confusing ones 2.4 Folders Based on File-Type A Very common practice is to keep different file types in different folder. One of the main mistake I see people writing code like this. DBI::dbGetQuery(conn, &quot; select count(*) as numbers, max(colname) as maxSome, min(colname) as minSome, from tablename group by col1, col2, col3 order by numbers &quot;) or codes like this. shiny::HTML( &quot; &lt;p&gt;At Mozilla, were a global community of&lt;/p&gt; &lt;ul&gt; &lt;!-- changed to list in the tutorial --&gt; &lt;li&gt;technologists&lt;/li&gt; &lt;li&gt;thinkers&lt;/li&gt; &lt;li&gt;builders&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;working together to keep the Internet alive and accessible, so people worldwide can be informed contributors and creators of the Web. We believe this act of human collaboration across an open platform is essential to individual growth and our collective future.&lt;/p&gt; &lt;p&gt;Read the &lt;a href=\\&quot;https://www.mozilla.org/en-US/about/manifesto/\\&quot;&gt;Mozilla Manifesto&lt;/a&gt; to learn even more about the values and principles that guide the pursuit of our mission.&lt;/p&gt; &quot; ) This is a bad coding style. Every time I see this type of code I realize that the person doesnt believe that either the code will change or It will be extended. There is nothing permanent in the programming neither code, nor frameworks and not even languages. If you keep this type of code in separate SQL files or html files you can easily edit them later, code will be more easier to read and there will be a separation of concern. Tomorrow if you need help in SQL or HTML, a UI designer or a Database designer can look into your code without getting bogged down in R code. It makes bringing more people to the team easier. 2.5 Creating Sub-folders On bigger projects simple folder structure tend to become more confusing. This is the main concern I have with the data folder every data scientist create and put all the files he has in that single folder. In these scenarios its better to have a sub-folder for different file types or may be different roles. Like you can create sub-folders based on file-types like CSVs, json, rds etc.. or you can even create sub-folders based on roles or needs Like all the data related to one tab or one functionality goes in one folder and so on There has to be a logical consistency in the folder structure. Its primarily for you to not get lost in your own folders that you created and secondary for people working with you to understand your code and help in places you need help. 2.6 Conclusion You have to create folders and everything has to be arranged in. Keep everything as organized as you keep your house. There are a certain principles that will help you in it. Create projects Name the files properly Create a file for different language create sub-folders wherever you fill necessary. "],["code.html", "Chapter 3 Code Structure 3.1 Create Sections 3.2 Order of Code 3.3 Indentation 3.4 Conclusion", " Chapter 3 Code Structure Once you have arranged the files and folders in a logical way then comes the fact that the code itself should be arranged in such a way that it feels easy to go through. Always remember Code is read more often then its written. Your world should revolve around this line. If you delegate your work while leaving your firm to someone else than the person who is handling your code should be able to understand everything you were trying to do. You will be in that position someday and you would wish your colleagues must have done the same. Even if you arent sharing your code to somebody one day when you will return back to the project after say 7 to 8 months you will be surprised to see the mess you created back then. With this in mind hope this will help you in your journey. 3.1 Create Sections Rstudio gives you an ability to create section by pressing ( ctrl + shift + R ), or you can create one by adding 4 dashes (-) after a comment or 4 hash symbol (#) after a comment. # some comment ---- # some comment #### Both are valid syntax. you can name your section in the comment. Same rule applies for the Rmarkdown documents as well you should always name your code chunk. # ```{r chunkname, cache=TRUE} It also helps you jump between sections (Shift+Alt+J). You can easily switch between sections and fold them at will. It helps you not only in navigation but keeping a layout of the entire code as well. A 800+ line files will look something like this. code chunk It makes your code beautiful to look and makes it maintainable in long run. 3.2 Order of Code When you write code there are standard practices that are used across the domain and you should definitely use them. These are simple rules that most beginners arent concerned about but the more experience you gain the more you start to realize the latent power of code organization. Here are a simple tip you should use. Call your libraries on top of code Set all default variables or global options and all the path variables at the top of the code. Source all the code at the beginning Call all the data-files at the top In this exact order. This coherence keeps all your code easy to find. Most annoying thing in debugging someone else code is finding path variables laid out inside functions. Please dont ever do that. That is not a good practices. Take a look at one of my file code order If you think you will forget it. There is a golden rule you must remember. Put all the external dependencies on top of your code. Everything that I mentioned above is external to the code in the file. In exact order. Libraries are external to the file. path variables other files apart from one you are working is external as well. databases and CSV Just by doing this you will be able to navigate better in your code. There arent any hard and fast rules for this only logical and sensible ones. Feel free to come up with your own layout that helps you in your analysis journey. 3.3 Indentation It goes without saying that indentation makes your code readable. Python is not the only language who has the luxury of indentation. No matter what language you work in your code should be properly indented so that we can understand the nature of code written. There are a few things you can understand about indentation. Maintain same number of spaces throughout your code. Your editor will help you out with it for sure but even if you are working on multiple editors. If you choose 2 spaces or 4 spaces as an equivalent of tabs you should stick to it. This is a golden rule you should never break. Then maintain the same style in your code. Look at the code below. foo &lt;- function( first_arg, second_arg, third_arg ){ create_file &lt;- readxl::read_excel(path = first_arg, sheet = second_arg, range = third_arg) } bar &lt;- function( first_arg, second_arg, third_arg ){ create_file &lt;- readxl::read_excel( path = first_arg, sheet = second_arg, range = third_arg ) } function foo is written horizontal and bar is written vertical. I would prefer styling of function bar but you may choose one and stick to it for entire project. Mixing styles is not considered good and might create problem in code review. There is a package by name grkstyle which implements vertical arrangement of code as mentioned above. You can look into it as well. 3.4 Conclusion In this chapter we discussed how to structure your code to make it more meaningful to read and easier to debug. The key takeaways from this chapter is: Create sections to write beautiful and navigable code Put those sections in a logical order Dont "],["func.html", "Chapter 4 Functions 4.1 Metadata or Information header 4.2 Pass everything through parameters 4.3 Use Return Statement 4.4 Keep a consistency in Return Type 4.5 Use Sensible Names for parameters too 4.6 use tryCatch 4.7 Write simple and unique functions 4.8 Dont load libraries or source code inside a function 4.9 Use Package::Function() approach 4.10 Conclusion", " Chapter 4 Functions I can not over emphasize the importance of functions. As a data scientist most of the time you will be writing functions. Only in couple of cases where you have to write complicated classes there too methods are nothing more than functions. Having solid grasp of best practices in functions is a must for everybody working in any language what-so-ever. Hopefully this chapter will help you in best coding practices for functions. 4.1 Metadata or Information header As I mentioned in the previous chapter it is a good practice to create sections for everything you do in R. functions are no exception to the rule. But along with that there are a couple of information you should write along with the function. I worked in a few MNC where we had to write metadata of every function before writing it down. It makes it easier for code-reviewer to understand you code and for the entire team to collaborate in the project. Its good for personal projects too Let me give you an example of what I mean by this. functions metadata You can see that if you are working on large teams or may be in big corporate settings where anybody can be reassigned to a different project. This data helps by identifying who wrote what and why. Examples of some important tags can be : written by written on parameters modified by modified on purpose descriptions You can create your own tags based on usecases and information needed for further scenarios. 4.2 Pass everything through parameters I have seen people writing functions with calling things from global environments. Take a look at the code below. foo &lt;- function(x){ return( x + y) } y &lt;- 10 foo(5) ## [1] 15 Here the value of foo is based on y which is not a part of the function instead its in global environment and function always have to search global environment for the object. consider these scenarios: bar &lt;- function(x, y){ y &lt;- y return( foo(x) ) } bar(5, 20) ## [1] 15 you would assume that the answer is 25 but its 15 because foo was created in the global environment and it will always look up value in global environment before anything else. This is called Lexical Scoping its okay if you dont know it. It is very confusing and could mess up your code at any point in time. I am an experienced R programmer I too have trouble getting my head around it. We can avoid all these situations by following the best coding practices that have been used in software industries for years. Function should be a self contained code which shouldnt be impacted by the outer world. Only is certain scenarios you allow to deviate from these rules but its a good coding practice none the less. now in the above example instead of relying on the global variable if I just had created a parameter for Y, my code would be simpler to write and easier to understand and I would not have to think about lexical scoping on every step. foo &lt;- function( x, y ){ return( x + y ) } bar &lt;- function( x, y ){ return( foo( x, y ) ) } bar(5, 20) ## [1] 25 Now this code returns 25 as we all expected and trust me the Y is still available in global environment but that doesnt impact the foo or bar at all. Now you can nest this function under multiple other functions and it will behave exactly like it should. There is a golden rule you should take away from this section. Avoid Global Variables at all costs. As much as possible pass everything through the parameters. That what they are for right !!! 4.3 Use Return Statement It is a very simple thing yet most of the R users never worry about it because R takes care of finer details for you. But return statements actually make your code easier to read. Suppose you have to review code return statement makes it easier to glance at the code and understand what is it doing. Almost all the programming languages are habitual with it. There are no good advantage I can tell you for a return statement other than readability. But just by following these practices R community as a whole could get more respect in programming community. So please use Return statements wherever possible. In Big MNC your code will never pass reviewer unless it has return statements. It also is good for functions that dont return anything you can just return true or false depending on the fact that the function ran without producing any error. Functions where you modify a data.table or where you change something in the database etc Its a standard practice in old programming languages like C++ and its a good practice indeed. We as a community should embrace these practices which will help us down the road. 4.4 Keep a consistency in Return Type Return type of a function should be consistent regardless of what happens in a code. You may assume this is so simple that it goes without saying who would in their sane mind return character vector instead of a numerical one and you would be right. But Things get complicated when people start to work in composite data types like Lists and Dataframes. Working with lists people get confused and forget this basic principle. I have seen function returning list of 2 elements on some conditions and 3 on other and 4 on some more. It makes it harder for users to work on those return values. Dont even get me started on dataframes. People write functions that do some magic stuff on dataframes and it sometimes return a dataframe of 10 columns, sometime 11 and sometime 8. Its such a common mistake to make. I understand if you are fetching a table from database and returning that same table via functions but during manipulations you must add empty columns or delete existing ones to make it consistent for the end user regardless of the conditions you have in the functions. 4.5 Use Sensible Names for parameters too Yet another simple thing but because most of us including me come from non computer science background we have a tendency to use names like x, y, z, beta, theta, gamma, string etc in our function parameters. I too am guilty of doing it in above code for foo and bar functions and in general. Many good and well established libraries in R are guilty of this sin too But in long run these words dont make much sense. Its hard to maintain that code and its hard for user as well. Lets take an example : join &lt;- function(x, y) x + y join(x = 12, y = 12) ## [1] 24 do you see that as a user who hasnt written or even looked at the code its already hard for him to understand what does x and y stands for. Only to get an error like this. join(x = &quot;mtcars&quot;, y = &quot;iris&quot;) ## Error in x + y: non-numeric argument to binary operator I know it is a stupid example but I see it every time in real code. When you only need numeric values why not include that information in the parameter name. something like: join &lt;- function(num_x, num_y) num_x + num_y It may not seem like much but this small change makes the life of the user so much better where he doesnt need to consult the documentation again and again. Their are other ways you can come up with sensible names in your code just to avoid this issue. Its a standard practice during code review to check the names and these names are never allowed in production environment. We will discuss more about names in another chapter but for now understand that parameter names are just as important as the name of the function and it should be meaningful and easier to understand. There should be some information buried in the name. 4.6 use tryCatch During deployment we would not like the shiny app or rest api or the chron job to fail. Its not a good experience to have for either the developer or the client. Best way to avoid it is wrap every function in a tryCatch block and log the errors. This way if you app has some bugs ( which every app does ). It will not crash and not destroy the experience of all the other people using it. Lets bring back the foo function : foo &lt;- function( x, y ){ tryCatch( expr = { return( x + y ) }, error = function(e){ print( sprintf(&quot;An error occurred in foo at %s : %s&quot;, Sys.time(), e) ) }) } foo(&quot;mtcars&quot;, &quot;iris&quot;) ## [1] &quot;An error occurred in foo at 2021-04-23 11:43:56 : Error in x + y: non-numeric argument to binary operator\\n&quot; Now imagine this line to be printed in a json file or inserted in a database with time stamp and other information instead of crashing the entire code only a particular functionality will not run which is huge. This is the difference between staying late on Saturday night to fix a bug vs telling them that I will fix it on Monday. To me that is big enough. 4.7 Write simple and unique functions Task of one function should be to do one thing and one thing only. There are numerous times when people assume they have written excellent code because everything is in a function. Purpose of a function is to reduce one unique task in a single line. If your function does multiple things then its a good Idea to Break your function into multiple one and then create a function which uses all of them. average_func &lt;- function( mult_params ){ tryCatch( expr = { ### # code to do stuff 1 ### ### # code to do stuff 2 ### }, error = function(e){ ### # code to log errors ### }) } Now imagine if today you are logging on a json file and tomorrow client wants to log it into a database. Changing it on every function is not only time consuming but dangerous in terms that now you can break the code. Now compare that to this code. stuff_1 &lt;- function(params_1){ ### # code to do stuff 1 ### } stuff_2 &lt;- function(params_2){ ### # code to do stuff 1 ### } log_func &lt;- function( log_params){ ### # code to log errors ### } best_func &lt;- function( mult_params ){ tryCatch( expr = { stuff_1() stuff_2() }, error = function(e){ log_func() }) } Here in this code every function has a clear responsibility and the main function is just a composite of multiple unique functions and it will be very easy to debug this code or change the functionality entirely. 4.8 Dont load libraries or source code inside a function You may assume nobody does it. But I have seen people doing it times and times again. foo &lt;- function( x ){ library(data.table) setDT(x) } bar &lt;- function( x ){ source(file = &quot;&quot;) } In functional programming terminology these functions are called impure functions . Function which change the global environment or some persistent changes are called impure functions. They require very delicate handling of the entire project. If I dont know how many packages and what version of them am I dependent on or what files have I loaded in my environment it makes debugging the code a lot more harder. By following this style of code you are making the debugging harder for your project. In fact I would argue that you should remove all the external dependencies from a function. Take this code for example. foo &lt;- function(x){ exl &lt;- readxl::read_excel(path = &quot;increased/external/dependency&quot;) ## ### do some data operation in the function ## return(exl) } bar &lt;- function( x, filepath = &quot;increased/external/dependency&quot;){ exl &lt;- readxl::read_excel(path = filepath) ## ### do some data operation in the function ## return(exl) } Foo and bar are both doing the same thing and both are relying on an external path for code to work. But because bar is clearly stating the filepath as an argument it is easier to change and adapt to new needs. If you still need to rely on a package call it from the main script not from a function. And if you absolutely need some functions of a package inside a particular function then use qualified imports and dont load the entire package. 4.9 Use Package::Function() approach R classes work differently than the traditional oops we all are aware of. Instead of object_of_class.Method syntax like other programmings have, we in R use method( object_of_class ) syntax. Where just by changing name collision is a pretty common thing. Its a pretty common thing in R that 2 packages use same function name for different operations. So Its always better to use qualified imports fancy name for mentioning which package does the function comes from. 4.9.1 You should load libraries in the order of their usage # library(&quot;not_used_much&quot;) # library(&quot;least_used&quot;) # library(&quot;fairly_used&quot;) # library(&quot;most_used&quot;) # library(&quot;cant_do_without_it&quot;) R uses the loading sequence to identify which function to give preference. Its usually to the last package loaded. Its called masking and its not a reliable technique but its better to arrange your code in that order for sake of simplicity. and Yes do not forget to mention the package name clearly. like prefer writing this always: # dplyr::filter() # stats::filter() # # ## instead of # # filter() For your small project this might not be a big deal but when multiple people are working on a code everybody might not be familiar with the packages you are using and they might not know that there is a naming collision between 2 functions. Its a best practice to explicitly tell R that this function comes from this package. It saves a lot of your time and for the person who is going to maintain your code too And it makes your debug experience a little better. 4.10 Conclusion In this chapter we discussed the best practices for writing functions in R. Here are the key takeaways from the chapter. write information about the function at top of it. avoid global variable and pass everything through parameters use return statement to end your function keep consistency in return types of a function use logical names for parameter use tryCatch in every function functions are supposed to do one thing and one thing only dont try to change global environment without letting the user know Use qualified imports with syntax package::functions every time possible "],["names.html", "Chapter 5 Naming Conventions 5.1 Popular naming conventions 5.2 Informative Names 5.3 Conclusions", " Chapter 5 Naming Conventions This chapter is crucial only for people to understand what are the bad naming practices we the R users have acquired over the years because of flexibility in the language. These names we give to the data or variables are not valid outside or R community and thus are subject to code reviews. You may even be asked to change name before deploying the code in production. The more bad naming practices the more time it takes you to fix them. Its a good practice to know the best practices for naming things in general. 5.1 Popular naming conventions There are 3 most famous naming conventions in programming community. They are used throughout the code in big projects to smaller ones. These are : 5.1.1 camelCase These names start with small letter and every subsequent word will start with upperCase letter like my name in camelCase would be written as vikramSinghRawat. All the functions in SHINY are camelCase. Its a great example of camelCase naming conventions. 5.1.2 PascalCase PascalCase is just like camel case but the only difference is the first letter is also UpperCase. My name would be written as VikramSinghRawat. 5.1.3 snake_case These names are all lower case with underscore between the name. My name in snake_case would be vikram_singh_rawat. TIDYVERSE is a great example of snake_cases. I really like the naming conventions of packages like stringi and quanteda. whenever you start a project you should choose one of the naming conventions for the entire team. So that no matter who writes the code there is a logical consistency in the names and anybody can predict the next letter. In many projects that I have worked camelCase were chosen for naming variables and PascalCase for methods or functions. I came to know later that this is a style many programming languages choose. Infact in langauges like golang if you write snake_cases linter will ask you to correct the name. But for SQL and R I would highly recommend snake_cases as many databases like postgres dont allow capital cases in column names you have to surround names in quotes if you need to use uppercase letters. In R tidyverse has gained huge momentum and now all the packages are following suite. Apart from that if your package can even tell what datatype are you working on that is a huge add on. Packages like stringi and quanteda are excellent example of this. And I would like to add no matter what you choose Please never include dot in any name. Thats a practice valid for only R code and it too is not accepted anywhere apart from R programming language. Overall choose a naming convention for a project and stick to it or ask your client if they have a preference on it. This saves you from trouble of code reviews. 5.2 Informative Names I may sound like a tidyverse fanboy ( I am not) but classes and data types in R are quite opaque so names of functions and objects should reflect precisely what they represent. There is no harm in using names with data-types before them # int_currency &lt;- 1:10 # chr_letters &lt;- letters # dt_mtcars &lt;- data.table::data.table(mtcars) # tbl_mtcars &lt;- tibble::tibble(mtcars) Above advice may be more useful for package developers but it can be used in broad scenarios even on a project where there are multiple working on a same project. If I know what datatype I am dealing with I dont have to go through the entire code and working on top of it becomes that much easier. You can use more descriptive names without data types in the beginning for your projects. Names like data, mainData, dummyVar, tempList etc.. should never be used in a project. Use more descriptive names like sales_data_2020, api_token, rate_of_interest etc 5.3 Conclusions Proper naming conventions will help collaboration in big teams and it makes the code easier to maintain. We should all strive for better names in the code. Its the hardest job to come up with new and unique names for a variable everytime you create one but this is the difference between an average programmer and a good one. Choose a naming convention and stick to it Dont include dots ( . ) in names Use informative names "],["envManagement.html", "Chapter 6 Environment Management 6.1 Avoid package dependencies when possible 6.2 renv for package management 6.3 config for external dependencies 6.4 Conclusion", " Chapter 6 Environment Management If you create a product today be it an API or Shiny App or Even a normal R-script. One thing you cant be sure of is to update the packages or the version of R. There are companies where you can not access different version of a package because multiple projects are relying on the same copy of the package. Its hard to update your package in these companies and you will need to get permissions from top admins to do so. Thus its better to rely on as less packages as possible and that too on the popular ones. But even after you have created a code you would want to keep a record of all the packages and their version as it is for that particular project. This is where environments come in handy. 6.1 Avoid package dependencies when possible Adding one tiny package to your work flow adds recursive dependency not only on the package that you imported but all the other package that your package is relying on and on packages that those packages rely on and so on and on I have also worked on organizations where you have to write an email to explain why you need a certain package to be installed on Rstudio cloud and why you cant get away with already installed packages. I hope you never have to work in such environment ever. But its a better software practice to keep dependencies as minimum as possible. Because each new thing brings a whole set of debugging issues and problems. Mostly this applies to the fact that you can get away with an lapply instead of relying on purrr::map . If your data is very small and you dont do much fancy stuff with it, May be you can get away with base R dataframes instead of tibble or data.table. With new R 4.1.0 you might as well can get away with base pipe |&gt; without magrittr pipe %&gt;%. These are certain examples I can think out of my head. But the implications are huge. If you can achieve something without relying on external dependencies be it a package or anything else you should always choose the one with less dependencies. 6.2 renv for package management There was a package called Packrat a few years ago I would have suggested you to use that always. But currently there is a package I have been using for over a year now by name renv. It does everything that you need to recreate your environment anywhere else. Basically you need to activate the package in your project. By using this command. renv::activate() Then take a snapshot of current project where it will record a list of all the packages used in your project by this command. renv::snapshot() and When you want to reproduce it on a docker container or a remote machine or any place else. You would simple need to run. renv::restore() and it generates a lock file with all the information about a project including the version or R and the versions of the packages used so at any time you can recreate the entire environment again. I could give you multiple ways of tackling the same problem. But this book is about the best possible one so this is it. You just need to use this package to solve almost all of your problems. 6.3 config for external dependencies There is a package called config that allows you to read yaml format in R. That is a standard practice to keep all the Credentials, tokens, API keys etc.. in a config file. There are many other ways you can secure credentials and everything but config is easiest amongst them all and you can use it for storing all the parameters and external path variables that your code requires. It could be an address to external file storage or anything else. Its good to keep all the variable your code requires outside the main code so that when you need to update them you dont need to change the entire code itself. Below is a snippet of config file from one of my project. default: datawarehouse: driver: Postgres server: localhost uid: postgres pwd: postgres port: 5432 database: master dockerdatabase: driver: Postgres server: postgres_plum uid: postgres pwd: postgres port: 5432 database: master filestructure: logfile: &quot;logs/logs.csv&quot; as you can see I havent only kept the passwords and user names but external files as well. Tomorrow if I have to change the logging file I will just have to update it here without opening any R code. It removes so much burden on reading the code again and again. Use it whenever possible. 6.4 Conclusion This chapter doesnt discuss much on concepts but the takeaways from the chapter are: Use as less packages as possible, it helps in code maintenance and debugging. Use renv for all the project you plan to maintain or keep for long term Use config to manage all the external dependencies your project have or might have "],["dataManagement.html", "Chapter 7 data Management 7.1 Keep a Copy or your Data 7.2 Dont use numbers for columns 7.3 Keep Meaningful and proper column names 7.4 Use Databases 7.5 Use Efficient Packages 7.6 Conclusion", " Chapter 7 data Management Because we are either data scientist or data engineers or data analyst or something close to data. R language is specifically tailored towards people working on data so I assume my guess above is correct to some degree. You should also understand a certain principles that go with data management in general. We wont be discussing the details but little tricks to work effectively with data. 7.1 Keep a Copy or your Data Always keep a copy of the original data. While working on the data you can mess up the data to a level where you cant bring back the old version of the data. So Always keep a backup copy of the data in some folder you like. This is the primary reason Excel has an undo button but Access dont. And R has a copy on modify syntax. You will have to explicitly tell R to change the copy of your data otherwise R will never mess up the original copy. Which is excellent for EDA and analysis in general. There is a huge section on copy-on-modify to come so at this moment you just need to know that keep a copy of original data somewhere safe. You will mess up the data big time so its okay to have a backup. 7.2 Dont use numbers for columns In R you can use numbers to refer to columns but just because you can doesnt mean you should. I have seen many people referring to columns of dataframe with numbers. Its not okay to do that because then you loose the context when you read it. mtcars[1, c(1,3:5, 8)] ## mpg disp hp drat vs ## Mazda RX4 21 160 110 3.9 0 Can you tell me precisely what columns am I working on? I have seen people writing this type of code in production and get rejected during code-review. Now compare this code to this one. mtcars[1, c(&quot;mpg&quot;,&quot;disp&quot;,&quot;hp&quot;,&quot;drat&quot;, &quot;vs&quot;)] ## mpg disp hp drat vs ## Mazda RX4 21 160 110 3.9 0 Which one is more readable and more clear to a new user or may be it will help you when after 6 months you will look back at it. Trust me you would not remember the column numbers at all. Its already hard to remember column names though. 7.3 Keep Meaningful and proper column names Most of the data we read is from either an excel file or a poorly designed database thus we see column names having Spaces and dots and all sort of funny things. Remember these rules they will help you even when you are designing a database and trying to name columns for a data base. Assume everything is case sensitive Use only lowercase letters, it will help you when you push it in DB Dont use special character except underscore Please dont use spaces at all There are functions in R like make.names but that too wouldnt help you naming your objects properly. When you have very less time go ahead with make.names but other than that please spend at least 1 to 2 hour naming the data. This small exercise will help you for the entire project. 7.4 Use Databases R has a limitation of RAM while handling data. Even though R has a way of dealing with larger than RAM data I will advice you to use a database instead. I would highly recommend disk.frame package if you are working on larger than RAM data. Its based on data.table and is pretty fast. But that too has limitation and its absolutely not a replacement for a database. There are tons of tools out there from disk.frame to ff to spark and what not but they arent a replacement for a real database. Even files stored on your system arent a proper replacement of a Database. FIrst and foremost requirement of being a data scientist is the ability to work with a database. Sooner or later you will need it. Please store your data in a proper database and learn some basics of database architecture. You dont need to learn everything but a little understanding of normalization will help you a long way in your journey. Use a database in all your project and save the credentials in a config file so that you can change them during production. Even if its just a prototype its okay to start with a database. So that you dont have to change the code once you migrate. The sooner you move your files to database the better it is for you. Using databases will bring advantages like: Early integration of DB in the project Understand the data types of your data as early as possible Call only the amount of data you need thus saving RAM Push computations to DB with packages like modeldb and dbplyr Basic computation like min, max and group by can be done more effectively Never worry about loosing your data or corrupting it There will always be a need for a database in any project what so ever. Please use it as early as possible. 7.5 Use Efficient Packages Working on data is the most memory consuming task. This is where you should understand the scale or your work. If you are developing something for only 5-10 people then you can get away with anything you do. Even though based on your app or api or product people will judge if R is a good language or not or should we be using R again. Than too I would not advice you to think much for such a small use case. Then when you start talking about 100 to 200 users active through out every second. In this case the difference between dplyr vs data.table can actually save the day. There are packages which can help you save memory and speed at the same time. I cant write an exhaustive list of all the packages but I can point you out to the packages and the situations where they will be helpful. 7.5.1 data.table Any list of efficient packages cant be started without this name. Its a package you must learn. We have created apps which were like 3-5 times faster than the python counterpart. The sole reason was we had data.table and they didnt. Despite what people say its easy to learn and easy to use in day to day analysis. It doesnt create a copy of the object and every function of this package is optimized for speed. Its faster than even spark for in memory datasets. Read that statement again if you didnt get how huge it is. I have a book in progress for people trying to learn data.table. This is the link you can read it: https://bookdown.org/content/2746/ 7.5.2 Matrix This package has the ability to create and work with sparse matrices. Sparse matrices save a lot of ram and lot of compute power too When you have any matrix where there are too many 0 or empty values. Sparse matrices are useful in those cases. Learn it and use it if possible. 7.5.3 disk.frame When you have data that is bigger than ram but still good enough that it can be stored on a disk. disk.frame is a best choice in those scenarios. For example you have a 15GB file on a 8 gb laptop than its an excellent candidate. It has its uses. However, I would still recommend to use a DB or spark for that matter but disk.frame has its own usage. 7.5.4 modeldb Its a package that can directly compute linear regression, logistic regression and some other calculations in the database. What it does is that it creates a long sql statement that basically computes the result of some algorithm and then sends it to a DB and gets the result back. Its excellent in scenarios when you have too much data and cant be brought back to R safely. Its better to use computation of SQL rather than bringing it to R. 7.5.5 dbplot Its a package for creating plot directly from a SQL database. It sends the computation directly to a database and gets the results needed to create a plot. Again its useful for circumstances where the data is more than R can handle in RAM. Its an excellent package for EDA and must be used by any serious practitioner. 7.5.6 sparklyr Lastly R has an interface to spark directly from R. Folks at Rstudio has made it so easy to use and install that you would feel you are working on a different programming language. Its definitely worth using when you have truly BIG Data. This is not an exhaustive list of all the packages available in R to handle memory efficiently. R has many more packages that have an ability to handle particular type of data or situation very well. But these are the must haves for anybody working with data. 7.6 Conclusion This chapter focus mainly focuses on good practices of working with data from R. And how to handle them with efficient packages. To go through what we learned so far. keep a copy of your data Dont use numbers for column names keep good column names that will help you in entire project use databases use efficient packages that can properly use computer resources available to you. "],["debug.html", "Chapter 8 Debugging 8.1 Write Unit Tests 8.2 Browser() and print() are your friend 8.3 Read the functions 8.4 Version Control System 8.5 Make small commits 8.6 Use curly brackets 8.7 Always use named parameters 8.8 Log the errors 8.9 Dont Use already used names 8.10 Use Simple code 8.11 Conclusion", " Chapter 8 Debugging Debugging is a part of being a programmer. You just cant escape it. Its a huge topic and it takes years of experience to master. To understand the limitations of a language and to understand the errors requires too much study and experience. This is something that you learn through experience. But there are certain thing which you can do to make your life easier while debugging. These are the tips I think everyone should follow for better debugging experience. 8.1 Write Unit Tests When you are writing a function and you have enough time to document the expected results of a function its a good practice to write down your expectations as a code. In R there is an excellent package called testthat for doing it. You dont need to create a package to test your code. You can simply test your functions by sourcing the entire file. Sky is the limit when you want to test your code. You have to decide how much time you have and how much are willing to write for testing. There has to be a minimum limit of tests for each sprint. That actually keeps you in track. Lets start with a very simple example. library(testthat) ## Warning: package &#39;testthat&#39; was built under R version 4.0.4 ## ## Attaching package: &#39;testthat&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## matches ## The following objects are masked from &#39;package:magrittr&#39;: ## ## equals, is_less_than, not foo &lt;- function(name){ if(length(name) == 0){ stop(&quot;please provide a character&quot;) } if(! is.na(name)){ if( !is.character(name) ){ stop(&quot;please provide a character&quot;) } x &lt;- sprintf(&quot;Hello %s, good morning&quot;, name) return(x) } else{ return(&quot;please provide a name&quot;) } } testthat::test_that( desc = &quot;testing foo&quot;, code = { ## check original result expect_equal( object = foo(&quot;vikram&quot;), expected = &quot;Hello vikram, good morning&quot; ) ## check results of NA expect_equal( object = foo(NA), expected = &quot;please provide a name&quot; ) ## check length of actual content expect_length( object = foo(&quot;vikram&quot;), n = 1 ) ## check Error on passing numeric code expect_error( object = foo(1) ) ## check Error on passing Date code expect_error( object = foo(Sys.Date()) ) ## check the error on NULL values expect_error( object = foo(NULL) ) } ) ## Test passed This is how you need to write test for a simple hello world example. To tell you the truth I started with just basic equal test but then I realized what if someone passes a number or a date what is someone passes a null value what is someone passes a NA So on and so forth I started modifying the function and this is how you will too This will make your function more secure and you will see less crashes. This is the reason I would like to begin this chapter with an emphasis on the fact that you should test your code. 8.2 Browser() and print() are your friend To me browser() function feels like a scene from movie matrix where you can stop everything around you and decide what is going wrong in the world. When you want to debug any function or a point where you assume that the error lies here, you should use browser() inside that function or script. Dont be scared of using browser() this function makes you familiar with your code and R itself. Browser is useful only when you know what function to look at. Print is your friend when you want to narrow down the candidates which are causing the error. Printing the objects on the console with help you understand what is happening inside a given function at the moment. Its very useful for interactive web apps where reproducing the error is a little tricky. A combination of print and browser can save a lot of your time. And yes make sure you delete the browser functions from your file. You can use ctrl + shift + f to search the entire project and every file in it for searching anything, including browser function. 8.3 Read the functions R gives you the ability to read the function and it comes very handy during debugging a function you have not written. You can view any function definition by running the function without () round brackets. like this quanteda::tokens ## function (x, what = &quot;word&quot;, remove_punct = FALSE, remove_symbols = FALSE, ## remove_numbers = FALSE, remove_url = FALSE, remove_separators = TRUE, ## split_hyphens = FALSE, include_docvars = TRUE, padding = FALSE, ## verbose = quanteda_options(&quot;verbose&quot;), ...) ## { ## tokens_env$START_TIME &lt;- proc.time() ## object_class &lt;- class(x)[1] ## if (verbose) ## catm(&quot;Creating a tokens object from a&quot;, object_class, ## &quot;input...\\n&quot;) ## UseMethod(&quot;tokens&quot;) ## } ## &lt;bytecode: 0x000000001c9864a8&gt; ## &lt;environment: namespace:quanteda&gt; And you can check which methods are available for which classes by using methods(class = &quot;dfm&quot;) ## [1] - ! $ $&lt;- %% ## [6] %*% %/% &amp; * / ## [11] [ [[ [&lt;- ^ + ## [16] all any anyNA Arith as.data.frame ## [21] as.dfm as.logical as.matrix as.numeric bootstrap_dfm ## [26] cbind cbind2 coerce coerce&lt;- colMeans ## [31] colSums Compare convert dfm dfm_compress ## [36] dfm_group dfm_lookup dfm_match dfm_replace dfm_sample ## [41] dfm_select dfm_smooth dfm_sort dfm_subset dfm_tfidf ## [46] dfm_tolower dfm_toupper dfm_trim dfm_weight dfm_wordstem ## [51] dim dim&lt;- dimnames dimnames&lt;- docfreq ## [56] docid docnames docnames&lt;- docvars docvars&lt;- ## [61] fcm featfreq featnames head initialize ## [66] is.finite is.infinite is.na kronecker length ## [71] log Logic Math Math2 meta ## [76] meta&lt;- ndoc nfeat ntoken ntype ## [81] Ops print rbind rbind2 rep ## [86] rowMeans rownames&lt;- rowSums show sparsity ## [91] Summary t tail topfeatures ## see &#39;?methods&#39; for accessing help and source code Or you can check how many classes have a method by same name with. methods(generic.function = &quot;print&quot;)[1:20] ## [1] &quot;print,ANY-method&quot; &quot;print,dfm-method&quot; ## [3] &quot;print,diagonalMatrix-method&quot; &quot;print,dictionary2-method&quot; ## [5] &quot;print,fcm-method&quot; &quot;print,sparseMatrix-method&quot; ## [7] &quot;print.acf&quot; &quot;print.AES&quot; ## [9] &quot;print.all_vars&quot; &quot;print.anova&quot; ## [11] &quot;print.ansi_string&quot; &quot;print.ansi_style&quot; ## [13] &quot;print.any_vars&quot; &quot;print.aov&quot; ## [15] &quot;print.aovlist&quot; &quot;print.ar&quot; ## [17] &quot;print.Arima&quot; &quot;print.arima0&quot; ## [19] &quot;print.AsIs&quot; &quot;print.aspell&quot; now most of these methods are hidden from general usage so you might not be able to view them. # textstat_lexdiv.dfm # will not work will produce an error # Error: object &#39;textstat_lexdiv.dfm&#39; not found quanteda.textstats::textstat_lexdiv ## function (x, measure = c(&quot;TTR&quot;, &quot;C&quot;, &quot;R&quot;, &quot;CTTR&quot;, &quot;U&quot;, &quot;S&quot;, &quot;K&quot;, ## &quot;I&quot;, &quot;D&quot;, &quot;Vm&quot;, &quot;Maas&quot;, &quot;MATTR&quot;, &quot;MSTTR&quot;, &quot;all&quot;), remove_numbers = TRUE, ## remove_punct = TRUE, remove_symbols = TRUE, remove_hyphens = FALSE, ## log.base = 10, MATTR_window = 100L, MSTTR_segment = 100L, ## ...) ## { ## measure &lt;- match.arg(measure, c(&quot;TTR&quot;, &quot;C&quot;, &quot;R&quot;, &quot;CTTR&quot;, ## &quot;U&quot;, &quot;S&quot;, &quot;K&quot;, &quot;I&quot;, &quot;D&quot;, &quot;Vm&quot;, &quot;Maas&quot;, &quot;MATTR&quot;, &quot;MSTTR&quot;, ## &quot;all&quot;), several.ok = TRUE) ## UseMethod(&quot;textstat_lexdiv&quot;) ## } ## &lt;bytecode: 0x000000001d241dd8&gt; ## &lt;environment: namespace:quanteda.textstats&gt; # works but the implementation is still hidden because method will be decided based on the class provided to the method at the exact moment of calculation But If you still want to know how to know the definition of a method of the class. Just use this code. getAnywhere(&quot;textstat_lexdiv.dfm&quot;) ## A single object matching &#39;textstat_lexdiv.dfm&#39; was found ## It was found in the following places ## namespace:quanteda.textstats ## with value ## ## function (x, measure = c(&quot;TTR&quot;, &quot;C&quot;, &quot;R&quot;, &quot;CTTR&quot;, &quot;U&quot;, &quot;S&quot;, &quot;K&quot;, ## &quot;I&quot;, &quot;D&quot;, &quot;Vm&quot;, &quot;Maas&quot;, &quot;all&quot;), remove_numbers = TRUE, remove_punct = TRUE, ## remove_symbols = TRUE, remove_hyphens = FALSE, log.base = 10, ## ...) ## { ## tokens_only_measures &lt;- c(&quot;MATTR&quot;, &quot;MSTTR&quot;) ## x &lt;- as.dfm(x) ## if (!sum(x)) ## stop(message_error(&quot;dfm_empty&quot;)) ## if (remove_hyphens) ## x &lt;- dfm_split_hyphenated_features(x) ## removals &lt;- removals_regex(separators = FALSE, punct = remove_punct, ## symbols = remove_symbols, numbers = remove_numbers, url = TRUE) ## if (length(removals)) { ## x &lt;- dfm_remove(x, paste(unlist(removals), collapse = &quot;|&quot;), ## valuetype = &quot;regex&quot;) ## } ## if (!sum(x)) ## stop(message_error(&quot;dfm_empty after removal of numbers, symbols, punctuations, hyphens&quot;)) ## if (any(tokens_only_measures %in% measure)) ## stop(&quot;average-based measures are only available for tokens inputs&quot;) ## available_measures &lt;- as.character(formals()$measure)[-1] ## measure &lt;- match.arg(measure, choices = available_measures, ## several.ok = !missing(measure)) ## if (&quot;all&quot; %in% measure) ## measure &lt;- available_measures[!available_measures %in% ## &quot;all&quot;] ## compute_lexdiv_dfm_stats(x, measure = measure, log.base = log.base) ## } ## &lt;bytecode: 0x000000001a9fe8a0&gt; ## &lt;environment: namespace:quanteda.textstats&gt; If you want to understand more of this learn OOPS in R. R has multiple object oriented systems and R is a highly Object oriented programming but the style is different from other languages. This book is all about best practices in R and thus we are not going to go deep into fundamentals of R programming here but this trick is worth knowing. These tricks will help you read code that is loaded on your environment but you have not written them. Reading someone elses code makes you a better coder. And it helps you understand why this code is breaking up. 8.4 Version Control System Use a version control system. For those who dont know, it means you can commit changes to a central repository and compare the changes anytime. GitHub and BitBucket are the most popular of these solution. Github allows a free account for every individual. Even for personal projects I would recommend you to use github or any version control system as such. For bigger projects use the one your organization recommends. This will help you compare changes you commit and go back to the old version that is up and running. It sounds easy but the power to compare what you changed in the code can help you pin point the error as quickly as possible. 8.5 Make small commits You should always use small commits. I have seen people who keep the code with themselves for days and change a thousand thing in the code before pushing it to github. I too am one of those people. Make small changes to your code and see if its working and then commit those changes. The smaller the commits the better debugging experience you have. Then its easier to roll back the changes and its easier to read the code to understand what might have caused this error. 8.6 Use curly brackets R gives you the ability to write code without {} but It makes your code harder to read and understand the blocks in segregation. I have seen people write code like this # if statements if( TRUE ) print(TRUE) else print(FALSE) ## [1] TRUE # loops for(i in 1:10) print(i) ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 # functions function(x) print(x) ## function(x) print(x) It sure makes your code look concise but only when its as small as what I wrote. Even then I would advice you to use curly brackets in all possible scenarios. Which helps specially when the code gets bigger or when you are using multiple of these statements together. Lets take this code for example. function(x) for(i in 1:10) if( i %% 2 == 0 ) print(TRUE) else print(FALSE) ## function(x) ## for(i in 1:10) ## if( i %% 2 == 0 ) print(TRUE) else print(FALSE) During big apps you are never sure of how many lines you need to write inside a function or a loop or a conditional and you have to update your code frequently. Without curly brackets it gets harder and harder to pin point the block that is causing the error. So in short using curly brackets help you understand the logic a little better and makes it easier to pin point the block thats causing the error. 8.7 Always use named parameters Lets compare two code in the below chunk. # Code with named parameters # # call_cognitive_endpoint( # endpoint = speech$get_endpoint(), # operation = &quot;models/base&quot;, # body = list(), # options = list(), # headers = list(`content-type` = &#39;audio/wav&#39;), # http_verb = &quot;POST&quot; # ) # ----------------- # Code without named parameters # # call_cognitive_endpoint( # speech$get_endpoint(), # &quot;models/base&quot;, # list(), # list(), # list(`content-type` = &#39;audio/wav&#39;), # &quot;POST&quot; # ) They are commented because we are only focusing on the structure of the function not the working of the function. Which of these codes looks more readable to you? Can you be sure in the below function that you have provided arguments in the right order and you are using the function exactly as it is meant to be used? This is why named parameters actually save time during debugging compared to unnamed one. This also makes code transferable which means that any new person in the team can quickly pick up where you left of, because may be you are so familiar with a function that you assume naming the parameter is not required at all but someone else might not be so much familiar with it. In big organizations where people come and go and anybody can be reassigned to the same code it helps to make it easy to read. And it will you in the long run when you will read your own code after say 1 to 2 years. 8.8 Log the errors This is an old advice I used in functions chapter where I asked you to use tryCatch in all the functions so that it doesnt brake during production. To extend that I would also argue to add logs specially the errors onto a json file or a database table. When you are running your code on your own computer you might get only 1000 bugs and you are prepared to handle those bugs in production. But suppose you create a shiny app that is used by 1000 more people. In those circumstances you will encounter which you might not be able to reproduce so easily and no body will tell you what bug your app still has. Logging errors is a standard practice in programming domain and its necessary for production grade apps, be it shiny app or a REST api. There are many packages available in R for logging I dont have a preference on any of them. Its good to use a database instead of json. 8.9 Dont Use already used names R allows you to override variable and function names that exist. But this is something you shouldnt do. Not even once. I get it when you dont know it collides with something but when you do you should avoid those names at all costs. Take fro example T and F are just variables which have TRUE and FALSE values stored in them. They are not a replacement for boolean values. T &lt;- FALSE myvar &lt;- TRUE if( myvar == T ) print(TRUE) else print(FALSE) ## [1] FALSE Here all your logic is gone because someone thought of renaming A variable T. Most common such error occurs on naming the object remember that these are all valid functions in base R. dt df data These are just a few examples where you can mess up your code very easily without realizing that you are doing something very wrong here. Just like other programming languages will throw an error if you use an already defined name and wont allow you to reuse it, you should treat R the same even though it will not throw an error and you might be lucky enough that it will never throw and error. But you should get into a habit of not reusing function and object names in R as well. 8.10 Use Simple code R gives you a lot of flexibility in coding style. You can write very succinct and precise code with R with highly complex methods. But try to spread your code in decent number of lines so that you can read it later on. Let me give you a very basic example. x &lt;- y &lt;- z &lt;- 1:10 ## or x &lt;- 3; y &lt;- 5; z &lt;- 8 ## or foo &lt;- function(x){ y &lt;&lt;- x } ## &lt;&lt;- is permisable only in very very very rare scenarios This is doable in R but doesnt mean you should do it. This code could easily have been split into 6 lines and it will increase the readability of your code. People from specially maths , finance, science etc background love to write complicated equations and they carry the same attitude to their coding style too However coding is more about code maintenance than about writing code and you cant hope that next person will have equal abilities that you do. Write simple and beautiful code is the best advice I can give in this entire chapter. This makes your life easier and of the people working with you. 8.11 Conclusion In this chapter we discussed multiple strategies of dealing with and avoiding debugging complexities. Hope if you follow most of these tricks you will feel that you are a better debugger than you were before and it will save you a lot of time in the process. To recap what we have learned today. Write Tests as much as possible Use print to point of where your code fails use browser to check the code always delete browser functions read the function You can even read hidden methods in R Use version control system make small commits use curly brackets in all your code Pass all arguments to a function through their name and not the position log the errors avoid already used function or object names write simple code avoid using T, F , &lt;&lt;- &amp; ; "],["types.html", "Chapter 9 Type System 9.1 Things you should know 9.2 Choose data types carefully 9.3 dont change datatypes 9.4 Future of type-system in R 9.5 Conclusion", " Chapter 9 Type System With Great power comes great responsibility  ( Uncle Ben ) Man who raised spider-man Despite what most people believe R too has data types. Every language tries to consume the memory space as efficiently as possible and for that they have pre-specified memory layouts that work almost all the same in every language. If you have worked on SQL databases the role of data types are exactly the same across all the languages. R just makes it easier to infer the data-type from your code so that you dont have to declare it specifically. primal data types for vectors in R are : logical numeric integer complex character raw then there are composite data types like date, posixct, even a dataframe is a list with some rules. 9.1 Things you should know There are a certain things you should know about data types in R. 9.1.1 R dont have scalar data types x &lt;- 10L x ## [1] 10 There is a reason [1] is written before the number 10. Its because unlike other languages R dont have scalar values. Everything in R is a vector. It may be a vector of length 1 or 1 billion but its all still a vector. This is one of the primary reason R works more like SQL ( as most data guy love ) and less like JAVA ( as most programmers love ). It gives huge speed to data manipulation as all the operations are more like In Memory Columnar Table. But in return when you are creating a webpage or an API or something where you need a scalar value to be updated again and again. R consumes more resources to do that kind of thing. microbenchmark::microbenchmark( vectorized = { x &lt;- rnorm(1e3) }, scalar = { y &lt;- numeric(1L) for(i in 1:1e3){ y[[i]] &lt;- rnorm(1) } }, times = 10 ) ## Unit: microseconds ## expr min lq mean median uq max neval cld ## vectorized 55 55.8 58.62 58.25 59.9 64.9 10 a ## scalar 3104 3401.7 3497.91 3513.45 3605.4 3838.6 10 b Even then I would ask you to go ahead with R because the difference will most probably in 1-2 milliseconds which will never impact your performance any serious way. But this is something you should remember that vectorized R is way faster than even python but scalar manipulations in R are a bit slower. Choose vectorized version of the code whenever possible even if you do a bit more steps in it, it will still be faster than the scalar versions. 9.1.2 Dates are basically integers under the hood. x &lt;- Sys.Date() class(x) ## [1] &quot;Date&quot; # &quot;Date&quot; as.integer(x) ## [1] 18740 This number means it has been 18 thousand 7 hundred days since 1970 which is roughly (365 * 50) 9.1.3 POSIXlt are basically lists under the hood unclass(x) ## [1] 18740 y &lt;- Sys.time() y &lt;- as.POSIXlt(y) class(y) ## [1] &quot;POSIXlt&quot; &quot;POSIXt&quot; # &quot;POSIXlt&quot; unclass(y) ## $sec ## [1] 59.88577 ## ## $min ## [1] 43 ## ## $hour ## [1] 11 ## ## $mday ## [1] 23 ## ## $mon ## [1] 3 ## ## $year ## [1] 121 ## ## $wday ## [1] 5 ## ## $yday ## [1] 112 ## ## $isdst ## [1] 0 ## ## $zone ## [1] &quot;IST&quot; ## ## $gmtoff ## [1] 19800 ## ## attr(,&quot;tzone&quot;) ## [1] &quot;&quot; &quot;IST&quot; &quot;+0630&quot; because it stores metadata along with it, use posixct whenever possible. 9.1.4 Integers are smaller than numeric x &lt;- sample(1:1e3, 1e8, replace = TRUE) class(x) ## [1] &quot;integer&quot; # [1] &quot;integer&quot; y &lt;- as.numeric(x) class(y) ## [1] &quot;numeric&quot; # [1] &quot;numeric&quot; object.size(x) ## 400000048 bytes # 400000048 bytes object.size(y) ## 800000048 bytes # 800000048 bytes See the difference yourself. Its about twice the size of the original integer vector. Its all because of datatypes. You should use integer only when you need one. There is a cool trick to let R know that you are creating an integer. Just add L at the end. x &lt;- 1 class(x) ## [1] &quot;numeric&quot; #[1] &quot;numeric&quot; y &lt;- 1L class(y) ## [1] &quot;integer&quot; # [1] &quot;integer&quot; Letter L at the end after a number will tell R that you want an integer. Please use integers when you need one. 9.1.5 define your datatypes before the variable i &lt;- integer(1e3) class(i) ## [1] &quot;integer&quot; length(i) ## [1] 1000 l &lt;- logical(1e3) class(l) ## [1] &quot;logical&quot; length(l) ## [1] 1000 n &lt;- numeric(1e3) class(n) ## [1] &quot;numeric&quot; length(n) ## [1] 1000 c &lt;- character(1e3) class(c) ## [1] &quot;character&quot; length(c) ## [1] 1000 Just like any other language even in R you can create an empty vector of a predefined length which are initialized at 0 or \"\" or FALSE based on the data types. Use this functionality when you want to create a column or vector you know nothing about except data type. Defining data-types beforehand is an excellent programming practice and we as R user should use it more often. It also removes burden on the compiler to try to guess the data-type. 9.1.6 lists are better than dataframe under a loop # x_dataframe &lt;- data.frame(x = 1:1e3L) # # for(i in 1:1e4L){ # x_dataframe$x[[i]] &lt;- i # } # This code will produce an error because you can&#39;t increase the row count of a dataframe like that. x_list &lt;- list(x = 1:1e3L) for(i in 1:1e4L){ x_list$x[[i]] &lt;- i } x_dataframe &lt;- as.data.frame(x_list) class(x_dataframe) ## [1] &quot;data.frame&quot; you can not create additional rows easily in data.frame. but all dataframes are lists under the hood with some additional rules. you can convert them to list run a loop and convert back to data.frame. Its not only efficient but its faster too 9.1.7 use lists whenever possible Other languages have structs to handle multiple object types. R have lists and lists are most versatile piece of data-type you will find across any language. There are tons of example like the one I provided above where lists are more efficient because they dont have any restrictions. In my personal use case I have seen people trying to put a square peg in round hole by using data.frames at places where a simple list will be more efficient and appropriate. Please use list as frequently as possible and remember, always opt for lower level data type for better memory management. 9.2 Choose data types carefully As we saw in examples above choosing the right data-type can mean a lot in x &lt;- seq.Date( from = as.Date(&quot;2000-01-01&quot;), to = Sys.Date(), length.out = 1e4) x &lt;- sample(x, 1e8, TRUE) y &lt;- data.table::as.IDate(x) length(x) == length(y) ## [1] TRUE object.size(x) ## 800000272 bytes object.size(y) ## 400000336 bytes as you can see the base date type consumes around twice the memory compared to IDate data type from data.table. It may be because one is using numeric data types under the hood and other is using integers under the hood and it makes a huge difference when you are working on big data, to understand the data types in R and properly map them to save more space on your RAM. Despite what most people say RAM and CPU are not cheap, throwing more processor on something should only be done when the code is properly optimized. I dont want you to be hyper optimize your code on every sentence you write but being aware of some best practices will surely help you along the way. Go to next section for speed optimization as well. There are many packages in R we will talk about that provide speed ups to the code and saves memory too We will talk about them later in the book. At this point all I can say is if R is slow or it crashes may be the data-type you have chosen is not right fit for the job. Try changing it and it will work just fine. 9.3 dont change datatypes R gives you an option to do this. x &lt;- &quot;Hello world&quot; print(x) ## [1] &quot;Hello world&quot; x &lt;- 1L print(x) ## [1] 1 Now you just assigned x as a character vector and then replaced x as an integer vector. This is something you can do but its something you should never do. changing the datatype of a vector is not recommended in any programming language unless you are trying to convert from one data-type to another. like : x &lt;- &quot;2020-01-01&quot; class(x) ## [1] &quot;character&quot; x &lt;- as.Date(x) class(x) ## [1] &quot;Date&quot; y &lt;- c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;) class(y) ## [1] &quot;character&quot; y &lt;- as.integer(y) class(y) ## [1] &quot;integer&quot; This and many operations like this where you know beforehand that you need to change the data-type is a good example of cases where you must change the data-types. So apart from cases where data-conversion is needed you should never change the data-types ever. Its a bad practice to do so. This is one of the scenario when you have the power but you mustnt use it. 9.4 Future of type-system in R Type system is important when you really want to save memory. Its specially true when you are dealing with huge volume of data and you want to save RAM more efficiently as possible, which is what an R users bread and butter. Its more like when you need it you really cant do without it. Every programming language is understanding this now. R is no exception to the rule. people are coming up with excellent theories on how to integrate a type system in R. Sooner or Later we will have a proper type system. Currently, There is a package called Typed by Antoine Fabri on CRAN. You can install it directly from cran. It will not give you speed benefits though because it doesnt talk to compiler directly but it surely will restrict people to using wrong data-types where you dont need them. Its helpful when you write functions where you only need vector of certain lengths and a certain type so that further operations can be optimized. Then there are packages like contractr by By Alexi Turcotte, Aviral Goel, Filip Kikava, Jan Vitek which can talk to compilers directly. Package is still in early development have no claims or documents available at their homepage at https://prl-prg.github.io/contractr/. But they are trying to insert type system through roxygen arguements above a function. I think this will be useful for package developers. It has a long way to go but we are thinking in right direction at least. For more information you should watch this video These packages are no substitute for an inbuilt type system and we may ignore it but we really need a type system going forward. Lets hope for the best and have our fingers crossed for the moment. 9.5 Conclusion In This chapter we focused on multiple data-types in R and how to save memory and CPU time by utilizing the best one in it. There are only a few but critical takeaways from this chapter : remember: R dont have scalars dates are integers POSIXlt should rarely be used use integers when you can define data-types beforehand use lists wherever possible choose data-types carefully Dont change data-types unless necessary In Future we will have a type-system and we should learn to love types early on. "],["reference.html", "Chapter 10 Pass By Value-Reference 10.1 Understanding the system 10.2 Copy on modify 10.3 for pass by reference 10.4 Conclusion", " Chapter 10 Pass By Value-Reference In programming we have a concept of how to pass a value to a function. If we can do away with modification of the object inside the function then its okay to pass the original object and let it change else we can create a copy of it and let the function modify it at will without effecting the object itself. understanding this concept is very crucial if you want to write efficient code. Lets dive deeper into it. 10.1 Understanding the system There are mostly 2 systems available for passing the objects from one function to another. Lets understand both of them. 10.1.1 Pass by Value This is when you create a copy of the original object and pass it to the function. Because you are actually passing just a copy to the function whatever you do to the object doesnt impact the original one. Lets check it by an example. x &lt;- list(y = 1:10) pass_by_value &lt;- function(x){ x$y &lt;- 10:1 } pass_by_value(x) x$y ## [1] 1 2 3 4 5 6 7 8 9 10 now x was passed to the function and modified yet it remains same because only copy of the object was passed to the function ( Well, not precisely but this is what we will discuss later). 10.1.2 Pass by reference This is when you pass the entire object as is. Basically you pass the pointer to the original object and now if you change the object you change the original copy of it. Lets check the same example again. x &lt;- new.env() x$y &lt;- 1:10 pass_by_value &lt;- function(x){ x$y &lt;- 10:1 } pass_by_value(x) x$y ## [1] 10 9 8 7 6 5 4 3 2 1 Now x was passed by reference and no copy was assigned to the function. So when you changed the object inside the function original object was changed. Hope you now understand practically what does the word mean. 10.2 Copy on modify R has no effective means to specify when to pass with value and when to pass with reference. And because there are only 2 ways to deal with this problem everybody assumes that R does create a copy of the object every time it passes the object through a function. But R has a different way of doing things which is called copy of modify. There are better blogs written over it and nuances are very peculiar which while writing code you shouldnt worry about much. I will try to simplify the concept from the practical point of you view so that you can use it in real life without much thought to it. R basically passes an object by references until you modify it. Lets check it live: mt_tbl &lt;- data.frame(mtcars) tracemem(mt_tbl) ## [1] &quot;&lt;00000000264396D8&gt;&quot; dummy_tbl &lt;- mt_tbl ## No tracemem yet mpg_col &lt;- as.character(mt_tbl$mpg) ## No tracemem yet mt_tbl[ (mt_tbl$cyl == 6) &amp; (mt_tbl$hp &gt; 90), ] ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## No tracemem yet subset( mt_tbl, cyl == 6) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## No tracemem yet tracemem is a function that will return memory address every time the object is copied. So far it didnt return anything even though it passed through so many functions and each of those functions must be using multiple functions internally. Yet no copy of the object was made. Because So Far we havent modified anything. now look at the code below. mt_tbl %&gt;% filter(cyl == 6, hp &gt; 90) %&gt;% group_by(gear) %&gt;% summarise(n()) %&gt;% select(gear) ## tracemem[0x00000000264396d8 -&gt; 0x00000000261c1a28]: initialize &lt;Anonymous&gt; filter_rows filter.data.frame filter group_by summarise select %&gt;% eval eval withVisible withCallingHandlers handle timing_fn evaluate_call &lt;Anonymous&gt; evaluate in_dir eng_r block_exec call_block process_group.block process_group withCallingHandlers process_file &lt;Anonymous&gt; &lt;Anonymous&gt; render_cur_session render_book render_book_script in_dir &lt;Anonymous&gt; &lt;Anonymous&gt; ## tracemem[0x00000000261c1a28 -&gt; 0x00000000261c1d98]: initialize &lt;Anonymous&gt; filter_rows filter.data.frame filter group_by summarise select %&gt;% eval eval withVisible withCallingHandlers handle timing_fn evaluate_call &lt;Anonymous&gt; evaluate in_dir eng_r block_exec call_block process_group.block process_group withCallingHandlers process_file &lt;Anonymous&gt; &lt;Anonymous&gt; render_cur_session render_book render_book_script in_dir &lt;Anonymous&gt; &lt;Anonymous&gt; ## # A tibble: 3 x 1 ## gear ## &lt;dbl&gt; ## 1 3 ## 2 4 ## 3 5 dplyr will change the data.frame to tibble and trigger tracemem This is one of the reason I absolutely love and recommend data.table to everybody. Which manages memory very efficiently its at par with any in memory table of a DB. If you are actually concerned about memory use data.table. new_tbl &lt;- mt_tbl %&gt;% filter(cyl == 6, hp &gt; 90) %&gt;% group_by(gear) %&gt;% summarise(n()) %&gt;% select(gear) ## tracemem[0x00000000264396d8 -&gt; 0x0000000021ada6b0]: initialize &lt;Anonymous&gt; filter_rows filter.data.frame filter group_by summarise select %&gt;% eval eval withVisible withCallingHandlers handle timing_fn evaluate_call &lt;Anonymous&gt; evaluate in_dir eng_r block_exec call_block process_group.block process_group withCallingHandlers process_file &lt;Anonymous&gt; &lt;Anonymous&gt; render_cur_session render_book render_book_script in_dir &lt;Anonymous&gt; &lt;Anonymous&gt; ## tracemem[0x0000000021ada6b0 -&gt; 0x0000000021ada760]: initialize &lt;Anonymous&gt; filter_rows filter.data.frame filter group_by summarise select %&gt;% eval eval withVisible withCallingHandlers handle timing_fn evaluate_call &lt;Anonymous&gt; evaluate in_dir eng_r block_exec call_block process_group.block process_group withCallingHandlers process_file &lt;Anonymous&gt; &lt;Anonymous&gt; render_cur_session render_book render_book_script in_dir &lt;Anonymous&gt; &lt;Anonymous&gt; now we are modifying the results somewhere and thus a copy is created. The actual rules are very very complicated. But in simple term as long as you dont modify any thing R doesnt create a copy and everything is passed down by reference. It impacts speed too Let check it by an example foo &lt;- function(x){ sum(x) } bar &lt;- function(x){ x[[1]] &lt;- 1 sum(x) } As you can see both the functions are identical the only difference is that in bar I am modifying the object while in foo I am not changing the object. Lets run a speed test x &lt;- rnorm(1e7) microbenchmark::microbenchmark( foo = foo(x), bar = bar(x), times = 10 ) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## foo 8.1400 8.2427 8.40834 8.3253 8.3786 9.4184 10 a ## bar 28.0347 28.8283 28.86869 28.8794 28.9966 29.4180 10 b As you can see the difference in time is because bar is creating a copy of the object. And you may assume that it will create a copy at every time you change a object and you will be dead wrong as R is smart enough to understand that It can get away with only single copy of the object. Lets create a function that changes more things in x and see the difference. bar_new &lt;- function(x){ x[[1]] &lt;- 1 x[[10]] &lt;- 10 x[[1e3]] &lt;- 1e3 sum(x) } microbenchmark::microbenchmark( foo = foo(x), bar = bar(x), bar_new = bar_new(x), times = 10 ) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## foo 8.1340 8.2359 8.36191 8.37340 8.4773 8.5193 10 a ## bar 28.4310 28.7630 36.02982 28.96915 29.2441 100.1655 10 b ## bar_new 28.5564 28.7673 29.09568 28.92150 29.1892 30.7277 10 b Now as you can see that while the function foo and bar have significant differences in performance, same is not true for bar and bar_new. Because bar_new too creates a copy but maintains that copy for the entire function. So R is smart enough to understand when to create a copy and when not to create a copy. Once a copy is created it is retained in R and R uses it smartly. We can gain speed and memory benefits by making sure all the modification is done inside a single function. So that R doesnt create much copies. Instead of using bar 3 times its better to use bar_new once. So that you dont copy it multiple times. See the difference for yourself. And thus try to keep all the modifications close and in as less functions as possible. microbenchmark::microbenchmark( bar = { bar(x) bar(x) bar(x) }, bar_new = bar_new(x), times = 10 ) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## bar 86.0658 86.8783 98.62073 87.29505 88.0293 148.2414 10 b ## bar_new 28.4509 28.5740 50.51256 28.79500 29.5109 200.7424 10 a best is to group these modifications together. So the gist of the matter is: R passes everything by reference until you modify it R creates a copy when you modify the object You should always keep all the Object modifications in same function 10.3 for pass by reference As I told you before R has no way of specifying when the object will be pass by reference and when it will be passed by value. And there are certainly times you wish you had passed it by value and certainly times when you wish you passed it by reference. When you modify something inside a function you create a copy of it. So take example of a loop inside and outside a function x &lt;- numeric(10) for(i in 1:10){ x[[i]] &lt;- rnorm(1) } x ## [1] 0.002156577 0.527250860 -1.011462254 -0.304480709 0.806666379 ## [6] -0.569644254 -0.647960995 0.468910360 -0.088721171 -0.227504696 It modifies the object in place. Now lets wrap it in a function and see what happens. x &lt;- numeric(10) foo &lt;- function(x){ for(i in 1:10){ x[[i]] &lt;- rnorm(1) } return(x) } foo(x) ## [1] 1.4545761 -0.9141887 0.5323786 -2.7380692 1.2628078 0.4769131 ## [7] -1.7960012 -2.2655847 -1.0728250 -0.6633129 x ## [1] 0 0 0 0 0 0 0 0 0 0 Now x is not modified because it is being modified inside a function. This is crucial at times when you are running a long job that might take hours to complete just to find an error in the middle. You might want to start the loop from the exact position you left off. With this sort of code you will not reach that. Lets generate an error in the code and uses bigger number. total_length &lt;- 1e2 set.seed(1) x &lt;- numeric(total_length) foo &lt;- function(number){ y &lt;- sample(1:total_length,1) for(i in 1:total_length){ number[[i]] &lt;- i if(y == i){ stop(sprintf(&quot;there is an error at %s&quot;, y)) } } return(number) } foo(x) ## You will get an Error ## Error in foo(x): there is an error at 68 If you run this code you will get an error at some number and x will still be the same. All the processing of code till that moment is lost for everybody. Which is not what you want if each iteration took just 2 minutes to run. This difference could mean hours in some scenarios. R has 4 datatypes that provide mutable objects or pass by reference semantics. R6 Classes environments data.table listenv I wouldnt recommend writing an R6 class just to run a simple loop, however if your use case is pretty complex R6 would be a valid solution for it. We already saw how environments can be used for pass by reference. But passing around environments is not a good idea it requires you to know too much about the language and be very careful with what you are doing hence I only prefer 2 approaches. One with data.table and other with listenv package. But their usecase is very different. One should be used where you are comfortable with lists are more suited while other should be used where data.frame or vectors are more suited for the task. Doing it for listenv is very easy. Its the same code with just the new listenv object. foo_list &lt;- function(list){ y &lt;- sample(1:total_length,1) for(i in 1:total_length){ list$x[[i]] &lt;- i if(y == i){ stop(sprintf(&quot;there is an error at %s&quot;, y)) } } } list_env &lt;- listenv::listenv() list_env$x &lt;- numeric(total_length) foo_list(list_env) ## Error in foo_list(list_env): there is an error at 39 Now again we got an errors but this time all the other changes have been saved in x. list_env$x ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 0 0 0 0 0 0 0 0 0 0 0 ## [51] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [76] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Same thing could be done in data.table as well. lets write a new function for doing it. data.table has 2 ways of looping through the vectors. With := operator which is slow but useful for more data insertion than one by one with set function which is faster where you need to insert data one by one. Lets use the second approach to write a function. x_dt &lt;- data.table::data.table(x = numeric(total_length)) foo_dt &lt;- function(dt){ y &lt;- sample(1:total_length,1) for(i in 1:total_length){ data.table::set( x = dt, i = i, j = &quot;x&quot;, value = i) if(y == i){ stop(sprintf(&quot;there is an error at %s&quot;, y)) } } } foo_dt(x_dt) ## Error in foo_dt(x_dt): there is an error at 1 Now just like again even though we got errors we can still check the ones that have been completed during the loop. x_dt$x ## [1] 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 So let me make things simpler. When you want to modify objects in place you need to use 1 of the 2 approach. When you are working on data.frames and vectors use data.table while when you are working on anything else, anything in general, use listenv approach. 10.4 Conclusion This chapter focused on how to save memory of your R program by using objects through reference and avoid creating copies of the object. Lets summarize what we have read so far. keep all the modifications of objects in a single function use pass by reference through listenv and data.table for saving memory avoid creating multiple copies of an object at all costs "],["releasememory.html", "Chapter 11 Release Memory 11.1 use rm() 11.2 use gc() 11.3 Cache / Store calculations 11.4 Conclusion", " Chapter 11 Release Memory Sometimes even after applying all the best techniques to save memory you still need more space. In those cases you need to understand that R has 2 techniques available to you. These techniques are mostly useful in different context. I mostly recommend using gc() inside a loop and rm() inside an ETL or R script. 11.1 use rm() rm function deletes any object from R and releases some memory that could be used for further operations. Its excellent for cases where you have a lot of intermediate results saved inside an R script that is supposed to run through an ETL. like this value &lt;- length(mtcars$mpg) sorted_mpg &lt;- sort(mtcars$mpg) median_values &lt;- sorted_mpg[value/2] + sorted_mpg[(value/2) + 1] median_mpg &lt;- median_values / 2 rm(list = c(&quot;value&quot;, &quot;sorted_mpg&quot;, &quot;median_values&quot;)) In these scenarios where you are running scripts like these and storing intermediate result for further processing its okay to use rm function. However when you wrap it inside a function only the final result stays and everything else is lost anyways so you dont need to use rm function at all. I would not advice anyone to use this function unless you are writing an ETL script on a data that is large enough that you would need space after every iteration. Even in those scenarios you can get away with not using intermediate variables for storing the results. But if you must and only and only in those very rare cases this function should be used. In broader cases Rs garbage collection will have you covered. 11.2 use gc() Garbage collection is a process by which a software or a program returns the memory back to OS or frees it for its own usage. R has significantly improved its memory management in recent years. All the blogs you read about R being slow and memory hungry are mostly written during 2014-2017. They arent true anymore. 11.2.1 R version 3.5 In R version 3.5 altrep was introduced which helps in saving a lot of memory in special cases like loops. Packages like vroom have achieved significant speed ups because now they didnt have to load all the data in memory and can only load as much as they need. If you want to read more about it you should visit this page https://svn.r-project.org/R/branches/ALTREP/ALTREP.html. 11.2.2 R version 4.0 In R version 4.0 ARC ( automatic reference counting )was introduced in R. Previously R used to keep a track of how many copies an object has with NRC ( named reference counting )and that had only 3 option, 0, 1 and 2; where I think 2 meant many. Now with ARC R can increase and decrease these numbers for keeping a better track of how many object points to the same memory address. You can read more about in a blog post here: https://msmith.de/2020/05/14/exploring-r-reference-counting.html#:~:text=The%20CRAN%20NEWS%20accompanying%20R,further%20optimizations%20in%20the%20future.. So the gist of the matter is that R has been improving performance and memory management for a very long time. Now coming back to the main point. R has a function by name gc to run a garbage collector and release the memory that is not needed anymore. Its useful to run it in mostly 2 situations. 11.2.3 Inside a heavy loop Loops are slow in R and sometime when are you doing heavy manipulation in a loop. R might consume more memory than its needed and will slow the further processes. This is the point I use a gc() inside a loop. gc_after_a_number &lt;- 1e2 for(i in 1:1e4){ ### heavy calculations if( i %% gc_after_a_number == 0){ gc() } } Here in the code above I am running a long loop but after every 100 iterations I am freeing up memory by running a gc() function. After how many iterations do you want to use a loop is totally dependent on what are you doing inside a loop. It could go from anywhere between 10 to 10 thousand depending upon the situation. It surely helps and I would recommend everyone to use it but only for very heavy calculations. 11.2.4 anything that takes more than 30 seconds If you know a function takes more than 30 second to execute. I would suggest you to run a gc() after it. It helps keep a check on Rs memory. Its useful even if you are running a shiny app or an API. If anything takes more than 30 second its worth running the gc() function because than adding a few milliseconds will not be your main problem for sure. 11.3 Cache / Store calculations Its a good practice in general to not repeat the same calculations over and over again. lets check that with simple function. Its a very famous problem of print foo when a number is divisible by 3 printing bar when its divisible by 5 and foobar when its divisible by both 3 and 5 or 15. foo &lt;- function( num ){ x &lt;- data.frame(y = 1:num) z &lt;- integer(num) for(i in 1:num){ if( x$y[[i]] %% 3 == 0 &amp;&amp; x$y[[i]] %% 5 == 0 ){ z &lt;- &quot;foobar&quot; } else if( x$y[[i]] %% 3 == 0){ z &lt;- &quot;foo&quot; } else if ( x$y[[i]] %% 5 == 0){ z &lt;- &quot;bar&quot; } else { z &lt;- i } } return(z) } Now look at the code above is it readable? what If I want to change the number of iterations. I will have to change it at 2 different places but thats okay because I have wrapped in a function. But the key point is that I am also calculating x$y[[i]] multiple times and If I have make changes to it I will have to change it in 4 places. Lets make it a little readable by storing the calculation and thus making our code readable and make it good enough that future changes can be done easily. bar &lt;- function( num ){ x &lt;- data.frame(y = 1:num) z &lt;- integer(num) for(i in 1:num){ curr_value &lt;- x$y[[i]] if( curr_value %% 3 == 0 &amp;&amp; curr_value %% 5 == 0 ){ z &lt;- &quot;foobar&quot; } else if( curr_value %% 3 == 0){ z &lt;- &quot;foo&quot; } else if ( curr_value %% 5 == 0){ z &lt;- &quot;bar&quot; } else { z &lt;- i } } return(z) } Lets check which is faster of the above implementations. microbenchmark::microbenchmark( foo(1e4), bar(1e4) ) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## foo(10000) 21.4537 22.3938 23.95090 23.49205 25.20715 35.3819 100 b ## bar(10000) 10.4650 11.0484 11.65059 11.33170 11.70855 18.7556 100 a We are saving almost half the time just because we are saving some intermediate calculations. You will be amazed to see how often can you do this. And it makes your code more performant more readable more adaptable more memory efficient Dont force the computer to calculate something it has already calculated. This simple thing has brought into life big concepts like mem-cache or reddis. R too has the ability to cache its results. There is a package by name memoise. It can store the results of a previous calculation in RAM. Its a classical case of RAM vs CPU trade off. You are using some RAM so that your CPU doesnt have to work over and over again. Lets try to calculate factorials to demonstrate how we can use it. fact &lt;- function(num){ if( num == 0){ return(1) } else { return( num * fact(num - 1) ) } } Now because being a recursive function, which you should avoid writing at all costs, It does the same computation over and over again. It goes something like this fact(0) * fact(1) fact(1) * fact(2) fact(2) * fact(3) fact(4) * fact(5) fact(5) * fact(6) . now its calculating same thing over and over again without caching it. It makes it a valid candidate for memoise. mem_fact &lt;- function(num){ if( num == 0){ return(1) } else { return( num * mem_fact(num - 1) ) } } mem_fact &lt;- memoise::memoise(mem_fact) This is how you write a simple memoise function. Lets compare the results. microbenchmark::microbenchmark( fact(100), mem_fact(100) ) ## Unit: microseconds ## expr min lq mean median uq max neval cld ## fact(100) 38.2 38.75 64.348 39.25 39.8 2396.9 100 a ## mem_fact(100) 52.6 54.45 313.855 55.95 57.9 25599.4 100 a In this case we havent achieved much performance over the base code but if we run this function multiple times it will just fetch the results from RAM and save computations in which case it will be worth the effort. We will go through example in next chapters where memoise will be faster too memoise is not the only package which does this sort of caching. There is a package by name cachem which is used in shiny and you will find some other products like redis which do the same thing but at scale. Its an important technique to know for saving both time and memory. 11.4 Conclusion In this chapter we learned how to free up memory for more usage in R. This should help you manage your R environment better. Lets see what we have studied so far. Dont use rm unless necessary rm should only be used in an ETL or a manual script Most of the time you can avoid rm use gc() inside heavy loops after every few iteration use gc() after every heavy transaction which takes more than a couple of seconds. Dont repeat calculations "],["speedtips.html", "Chapter 12 Some Tips to make R code faster 12.1 Use Latest version of R 12.2 Benchmark and profiling the code 12.3 Algorithm matters more than language 12.4 Read the function 12.5 Use Conditionals to break computations 12.6 Use Faster packages 12.7 Some pointers 12.8 Export Other languages 12.9 Conclusion", " Chapter 12 Some Tips to make R code faster Make it work, then make it beautiful, then if you really, really have to, make it fast. 90 percent of the time, if you make it beautiful, it will already be fast. So really, just make it beautiful!  Joe Armstrong In IT sector speed is very important. People rewrite tons of algorithms back again in c, c++, go and java. Just to gain that milliseconds or may be microseconds performance over one another. When you compared to these languages R is a very slow language. If you really need to get nanosecond level of optimization in R that are not possible without going to Rcpp; which by the way is a very easy wrapper for R user around C++. But still R code can be optimized to a level where you can get production level efficiency in R without too much trouble. And R is not slow compared to interpreted languages like python, Javascript, ruby etc 12.1 Use Latest version of R Each iteration of R is improving something to gain more speed with less and less memory. Its always useful if you need more speed to switch to Latest version of R and see if you get any speed gains. In general if you are using old versions of R or old functions that are deprecated and are no longer recommended by switching to a new version or new methods you will get a speed advantage for sure. Constant criticism that R is slow has made R to work in this respect and R is evolving according to the needs of time. There is not much to add here. If possible use the latest version, packages or methods mostly they might have more speed. 12.2 Benchmark and profiling the code R is very obscure language there are no direct rules for speed gains. You might think you are making the code fast but in turn you could make it slow. The worst part about R is that you can write very very very slow code in R without realizing what are you missing. same R code can run 1K times faster when optimized. R is a very easy language to write slow code in. This is something you should keep in mind while writing the code. This is the reason you should benchmark your options, It may not give you much speed improvement, it may not give you any speed improvement at all. If you want to optimize R you must learn to benchmark the options. I would not go in details but microbenchmark is the best package for this task. Other packages have too many assumptions. Sometime you may assume your code is slow because you have not used a best practice but your code might be slow for an entirely different reason. To figure out all the parts of code in comparison to one another profiling works like charm. You should use it especially before making your code live. You could save so much of the CPU time just by using profiler and evaluating if you are okay with the speed or you would actually want to sit for hours to save a few milliseconds. It doesnt matter in an ETL script mostly but it matters in an API or Shiny app. You have to decide whats okay and profiling your code will help you with it. 12.3 Algorithm matters more than language I see many people who write R for a single project and than because they cant make it run fast they switch to other languages like python mostly because they have read a few blog post written 5 to 10 years ago on how slow R is. In IT sector speed matters most and I would agree that if you could save a few milliseconds just by following a few basic rules please do that. Because when you create a shiny App or a Plumber API which many people hit at the same time every millisecond counts. But Dont get occupied by optimizing your code before it starts to work. Let me give you a basic structure, if your API can handle 40-50 requests per second on a single core you are at very high speed. Which means 20 to 30ms for each request. Usually network latency and disk caching and talking to DB etc takes more time. APIs mostly go from 200 to 500 ms per second in complex web apps. And R may not be the fastest language in the world but it sure can reach this level with minimum effort possible. Rest is all about scaling your app. So before you think about switching the language or saying that R in general is a slow language ask yourself have you optimized your code yet. Because if you dont optimize your code it doesnt matter what language you write it in. It will always be slow. Let me beat c++ with R and show you what I mean. Lets understand this by a very simple example. Lets start with the worst way you could code in any language called recursive functions and mark my words Never Use Recursive Functions. You are always better off without them. Lets try to see if we can find the good old fibonacci numbers and first 30 of them. We will write them in R and C++ alike. recurse_fib_r &lt;- function(fnum){ if(fnum &lt;= 1) { return(fnum) } else { return( recurse_fib_r(fnum-1) + recurse_fib_r(fnum-2) ) } } #include &lt;Rcpp.h&gt; //[[Rcpp::export]] int recurse_fib_rcpp(int fnum){ if(fnum &lt;= 1) { return(fnum) ; } else { return recurse_fib_rcpp( fnum - 1 ) + recurse_fib_rcpp( fnum - 2 ) ; } } lets compare both the functions now. microbenchmark::microbenchmark( mapply( recurse_fib_rcpp, 1:30 ), mapply( recurse_fib_r, 1:30 ), times = 10 ) ## Unit: milliseconds ## expr min lq mean median ## mapply(recurse_fib_rcpp, 1:30) 6.9464 7.0699 7.22704 7.15515 ## mapply(recurse_fib_r, 1:30) 2682.1853 2734.9644 2751.66857 2750.45405 ## uq max neval cld ## 7.1904 8.3227 10 a ## 2763.9522 2849.0588 10 b While c++ is still at milliseconds R has reached to seconds and that too for only 30 fibonacci numbers. This is not acceptable at any level you work on. Even if you are writing basic scripts this is not permissible to be sitting on your computer at all. Lets try to save memory by caching the results of previous operations. Lets try to save computation by using memoise package for caching intermediate results. mem_fib_r &lt;- function(fnum){ if(fnum &lt;= 1) { return(fnum) } else { return( memoised_fib_r(fnum - 1) + memoised_fib_r( fnum - 2) ) } } memoised_fib_r &lt;- memoise::memoise(mem_fib_r) Lets compare it with c++ microbenchmark::microbenchmark( mapply( recurse_fib_rcpp, 1:30 ), mapply( memoised_fib_r, 1:30 ), times = 10 ) ## Unit: milliseconds ## expr min lq mean median uq max ## mapply(recurse_fib_rcpp, 1:30) 6.3374 6.7563 6.93663 6.9731 7.1729 7.2233 ## mapply(memoised_fib_r, 1:30) 1.7787 1.8426 3.14454 1.9202 2.0159 14.1090 ## neval cld ## 10 b ## 10 a We have beat the c++ just by a very simple optimization. But If we write a simple function that doesnt use recursion we can still get better performance. Lets write a better algorithm by writing a loop. save_fib_r &lt;- function(fnum){ fnum &lt;- fnum + 1 vec &lt;- integer(fnum) vec[[2]] &lt;- 1 if(fnum &gt; 2){ for(i in 3:fnum){ vec[[i]] &lt;- vec[[ i - 1]] + vec[[ i - 2]] } } return(vec[[fnum]]) } Lets compare the results. microbenchmark::microbenchmark( mapply( recurse_fib_rcpp, 1:30 ), mapply( save_fib_r, 1:30 ), times = 10 ) ## Unit: microseconds ## expr min lq mean median uq max ## mapply(recurse_fib_rcpp, 1:30) 6272.9 6485.9 6634.44 6758.7 6797.2 6836.4 ## mapply(save_fib_r, 1:30) 104.0 105.5 623.77 112.3 119.0 5233.2 ## neval cld ## 10 b ## 10 a Now we are beating it with around 40x speed or more. But I think we can do better. This functions is vectorized but I am only asking for a single number I am doing the same calculations multiple time inside mapply function. If instead of using mapply I call the entire vec directly I will save computation. save_vec_fib_r &lt;- function(fnum){ vec &lt;- integer(fnum) vec[[2]] &lt;- 1 if(fnum &gt; 2){ for(i in 3:fnum){ vec[[i]] &lt;- vec[[ i - 1]] + vec[[ i - 2]] } } return(vec) } Now lets compare the differences microbenchmark::microbenchmark( mapply( save_fib_r, 1:1e3 ), save_vec_fib_r(1e3), times = 10 ) ## Unit: microseconds ## expr min lq mean median uq max ## mapply(save_fib_r, 1:1000) 52122.1 55235.7 56431.17 55892.10 58677.1 58916.7 ## save_vec_fib_r(1000) 117.0 122.7 618.99 124.55 129.0 5066.8 ## neval cld ## 10 b ## 10 a Now the difference is not only huge but we are calculating 1000 fibonacci numbers instead of just 30 we were working on previously. But I agree that I didnt gave c++ a chance. Languages have come and gone c++ have stood the test of time. Its the fastest language there is and R is nowhere close to it. I was just trying to compare an optimized version with an un-optimized one. Lets rewrite this same function in Rcpp just to see how far we are from the fastest programming language. #include &lt;Rcpp.h&gt; using namespace Rcpp; //[[Rcpp::export]] IntegerVector fib_rcpp(int fnum){ IntegerVector vec(fnum); vec[0] = 0; vec[1] = 1; if(fnum &gt; 1){ for(int i = 2; i &lt; fnum; ++i) { vec[i] = vec[ i - 1] + vec[ i - 2]; } } return vec; } microbenchmark::microbenchmark( fib_rcpp(1e5), save_vec_fib_r(1e5) ) ## Unit: microseconds ## expr min lq mean median uq ## fib_rcpp(1e+05) 174.5 193.601 224.157 210.851 237.101 ## save_vec_fib_r(1e+05) 10620.3 11538.701 11712.990 11718.851 11869.601 ## max neval cld ## 1155.901 100 a ## 19746.001 100 b So optimized R is about 28-30 times slower than the optimized rcpp code, which is a very good spot to be at. And to top it off now we are working on 1e5 numbers and that too within milliseconds in R. I wouldnt loose a sleep over it. So always try to optimize the language before going anywhere else. R is the most easiest language to write slow code in but the code can be optimized to 1000x easily with a few hacks like I just did. 12.4 Read the function You may assume just because you are using a base function that would be optimized to the core and thus it will be fastest solution out there. However thats far from truth sometime base R functions are overextended to check a few basic assumptions. You should get into a habit of reading the code. Its beneficial for debugging and for optimization as well. Let start small, R has a built in function by name replace and it does exactly what is intended from it it replaces a value from an index of a vector. But lets read it. replace ## function (x, list, values) ## { ## x[list] &lt;- values ## x ## } ## &lt;bytecode: 0x000000001cee2b50&gt; ## &lt;environment: namespace:base&gt; Its no more than a basic function you could write yourself. Lets check another one of my favorite function. stopifnot ## function (..., exprs, exprObject, local = TRUE) ## { ## n &lt;- ...length() ## if ((has.e &lt;- !missing(exprs)) || !missing(exprObject)) { ## if (n || (has.e &amp;&amp; !missing(exprObject))) ## stop(&quot;Only one of &#39;exprs&#39;, &#39;exprObject&#39; or expressions, not more&quot;) ## envir &lt;- if (isTRUE(local)) ## parent.frame() ## else if (isFALSE(local)) ## .GlobalEnv ## else if (is.environment(local)) ## local ## else stop(&quot;&#39;local&#39; must be TRUE, FALSE or an environment&quot;) ## E1 &lt;- if (has.e &amp;&amp; is.call(exprs &lt;- substitute(exprs))) ## exprs[[1]] ## cl &lt;- if (is.symbol(E1) &amp;&amp; E1 == quote(`{`)) { ## exprs[[1]] &lt;- quote(stopifnot) ## exprs ## } ## else as.call(c(quote(stopifnot), if (!has.e) exprObject else as.expression(exprs))) ## names(cl) &lt;- NULL ## return(eval(cl, envir = envir)) ## } ## Dparse &lt;- function(call, cutoff = 60L) { ## ch &lt;- deparse(call, width.cutoff = cutoff) ## if (length(ch) &gt; 1L) ## paste(ch[1L], &quot;....&quot;) ## else ch ## } ## head &lt;- function(x, n = 6L) x[seq_len(if (n &lt; 0L) max(length(x) + ## n, 0L) else min(n, length(x)))] ## abbrev &lt;- function(ae, n = 3L) paste(c(head(ae, n), if (length(ae) &gt; ## n) &quot;....&quot;), collapse = &quot;\\n &quot;) ## for (i in seq_len(n)) { ## r &lt;- ...elt(i) ## if (!(is.logical(r) &amp;&amp; !anyNA(r) &amp;&amp; all(r))) { ## dots &lt;- match.call()[-1L] ## if (is.null(msg &lt;- names(dots)) || !nzchar(msg &lt;- msg[i])) { ## cl.i &lt;- dots[[i]] ## msg &lt;- if (is.call(cl.i) &amp;&amp; identical(cl.i[[1]], ## quote(all.equal)) &amp;&amp; (is.null(ni &lt;- names(cl.i)) || ## length(cl.i) == 3L || length(cl.i &lt;- cl.i[!nzchar(ni)]) == ## 3L)) ## sprintf(gettext(&quot;%s and %s are not equal:\\n %s&quot;), ## Dparse(cl.i[[2]]), Dparse(cl.i[[3]]), abbrev(r)) ## else sprintf(ngettext(length(r), &quot;%s is not TRUE&quot;, ## &quot;%s are not all TRUE&quot;), Dparse(cl.i)) ## } ## stop(simpleError(msg, call = if (p &lt;- sys.parent(1L)) ## sys.call(p))) ## } ## } ## invisible() ## } ## &lt;bytecode: 0x00000000156c4688&gt; ## &lt;environment: namespace:base&gt; Again its basic R function, a huge one though. I wouldnt recommend you to rewrite it but if you just need a stop call on a basic condition you will be writing faster code with just simple if and stop function. Lets see that again in other base code. ifelse ## function (test, yes, no) ## { ## if (is.atomic(test)) { ## if (typeof(test) != &quot;logical&quot;) ## storage.mode(test) &lt;- &quot;logical&quot; ## if (length(test) == 1 &amp;&amp; is.null(attributes(test))) { ## if (is.na(test)) ## return(NA) ## else if (test) { ## if (length(yes) == 1) { ## yat &lt;- attributes(yes) ## if (is.null(yat) || (is.function(yes) &amp;&amp; identical(names(yat), ## &quot;srcref&quot;))) ## return(yes) ## } ## } ## else if (length(no) == 1) { ## nat &lt;- attributes(no) ## if (is.null(nat) || (is.function(no) &amp;&amp; identical(names(nat), ## &quot;srcref&quot;))) ## return(no) ## } ## } ## } ## else test &lt;- if (isS4(test)) ## methods::as(test, &quot;logical&quot;) ## else as.logical(test) ## ans &lt;- test ## len &lt;- length(ans) ## ypos &lt;- which(test) ## npos &lt;- which(!test) ## if (length(ypos) &gt; 0L) ## ans[ypos] &lt;- rep(yes, length.out = len)[ypos] ## if (length(npos) &gt; 0L) ## ans[npos] &lt;- rep(no, length.out = len)[npos] ## ans ## } ## &lt;bytecode: 0x0000000012de08f8&gt; ## &lt;environment: namespace:base&gt; You might think ifelse is an optimized function in base R which is faster and optimized at the compiler level or interpretator level. But in fact if you read the function carefully and realize that its wasting on checking if you are passing an atomic vector and you are better off just using the last 5-6 lines of the function for a faster result. These are just the basic examples I could think out of my mind and there are tons of such example where you could optimize a function just by reading it and realizing you might not need so much of hassle in the first place. You could avoid meta-programming or non-standard evaluation of these functions just by rewriting some of its parts yourself. Reading the function will give you insight into whats it trying to do and is it fast enough for your use case and can you optimize it yourself. This thing applies to package level codes as well. And sometimes, not always its good to rewrite a custom solution for yourself. 12.5 Use Conditionals to break computations Somebody told me that the key to going BIG is doing as LITTLE as possible. R understands it and does that by default. Lets check an example where this is true. foo &lt;- function(x){ if( x == 10){ bar() } print(x) } foo(1) ## [1] 1 This function worked perfectly fine even though we havent created any function by name bar(). R havent evaluated that expression at all. In most of the other programming languages this is not possible at all. We will get an error During compilation of the function. But R lets you go away with this. And then there are other example like this if( TRUE || stop()) print(&quot;TRUE&quot;) ## [1] &quot;TRUE&quot; Here because || has lazy evaluation and doesnt read the next option unless first one is false you save time by doing it especially for complex equations. lazy_return &lt;- function(x){ y &lt;- 1:x return( y ) } eager_return &lt;- function(x){ y &lt;- integer(x) for(i in 1:x){ y[[i]] &lt;- i } return(y) } microbenchmark::microbenchmark( lazy_return(1e5), eager_return(1e5) ) ## Unit: nanoseconds ## expr min lq mean median uq max neval ## lazy_return(1e+05) 200 301 12383.9 601 1800 1133900 100 ## eager_return(1e+05) 3381201 3883402 3927722.0 3922951 3987851 6825001 100 ## cld ## a ## b ALTREP based vectors have the same effect as well. R hasnt started evaluating y in function lazy_return while it evaluated them in eager_return. Just because y are not evaluated in lazy_return while they are evaluated in the eager_return. x &lt;- lazy_return(1e3) y &lt;- eager_return(1e3) .Internal(inspect(x)) ## @0x0000000026cc82f0 13 INTSXP g0c0 [REF(65535)] 1 : 1000 (compact) .Internal(inspect(y)) ## @0x000000002759a630 13 INTSXP g0c7 [REF(2)] (len=1000, tl=0) 1,2,3,4,5,... People normally assume that for loops in R are very slow this is the reason eager_return is slow. But when you inspect the internal structure of both x and y you will see that x is a compact representation of number 1:1000 and thus its not using memory and its not even evaluated yet. While Y is a full fledged vector with all the numbers from 1 to 1000 stored in it. It consumes memory and time. While all the other calculations on these vectors will work exactly the same and it might not take as much time as you would assume. microbenchmark::microbenchmark( altrep = x + x, full_vector = y + y ) ## Unit: microseconds ## expr min lq mean median uq max neval cld ## altrep 1.7 1.801 2.04401 2.0010 2.102 4.801 100 a ## full_vector 1.7 1.801 2.05793 1.9015 2.201 6.701 100 a Other such example would be not evaluating a promise until its needed. In very simple terms R divides every operations into promises that have to be evaluated at a later stage and only when they are needed, Otherwise they might not be evaluated at all. func &lt;- function(x){ function(){ eager_return(x) } } x &lt;- 10 a &lt;- func(x) x &lt;- 12 a() ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 instead of 10 which we used when we created a it actually produced 12 because it didnt started evaluating the values until it actually had to evaluate the function. This function worked fine even though there is no eval_foo and eval_bar function created here. R simply ignores the evaluation until needed. If the value lies above 10 or below 0 it will create an error saying there is no such function available for R to evaluate. lazy_func(11) ## Error in lazy_func(11): could not find function &quot;lazy_func&quot; Now we got the error. This can be used effectively for reducing burden on the CPU by running only the code that is necessary. Its a general practice that everyone uses even in different programming languages. So you can use this advice multiple ways, but try to break your code in multiple chunks and evaluate only that you need at the moment. It helps speed up the code a lot. 12.6 Use Faster packages R is not among fast languages. Its a well known fact. Its meant to write scripts for data science, statistics and analytics etc This is the reason R uses other languages for faster computation. You can use C++, C, Java and rust directly from R. And because R have been around for almost 30 years somebody somewhere must have created a package you can use to solve your problem. In this regards the knowledge of existing packages to solve a problem in hand is actually better. This is something you can learn through experience or through guru google. There are multiple such packages that you can use to speed up your workflow. data.table xts Matrix collapse RFast Out of my head I can only think of these very few but general libraries, which are very fast and actually can save you a lot of time. But then there are some general libraries that have equally fast alternatives too Like tidytext vs quanteda. Quanteda uses the memory more efficiently by using sparse matrices and they use c++ functions under the hood, while Tidytext mostly uses tibble which explodes in size very quickly. And then there might be other packages that could have a faster alternative. Like I have been using qs package a lot for storing and retrieving data from disk, Its actually faster than base R. 12.7 Some pointers 12.7.1 use [[ instead of [ when you can x &lt;- data.frame(y = 1:1e4) microbenchmark::microbenchmark( &quot;[[&quot; = for(i in 1:1e4){ x$y[[i]] &lt;- i * 2 }, &quot;[&quot; = for(i in 1:1e4){ x$y[i] &lt;- i * 2 }, times = 10 ) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## [[ 95.5579 95.9190 97.13521 96.48695 98.6158 99.5358 10 a ## [ 95.7270 96.3569 110.44840 98.88455 99.3484 219.8091 10 a The difference is just in milliseconds but there is a difference non the less. If you use it precisely you might save a few millisecond when you need them with just a basic understanding that this too could help you at times. Best way to navigate a nested list is through [[ function by passing a character vector. Take this as an example x &lt;- list( y = list( z = list( a = 1 ) ) ) x[[c(&quot;y&quot;, &quot;z&quot;, &quot;a&quot;)]] ## [1] 1 or if you want to extract just z then x[[c(&quot;y&quot;, &quot;z&quot;)]] ## $a ## [1] 1 Its pretty helpful when you are working on json objects. take this for an example. x &lt;- list( y = list( list( z = 1 ), list( z = 2 ), list( z = 3 ), list( z = 4 ), list( z = 5 ) ) ) lapply(x$y,`[[`, &quot;z&quot;) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 2 ## ## [[3]] ## [1] 3 ## ## [[4]] ## [1] 4 ## ## [[5]] ## [1] 5 These tricks will help you get some juice out of your machine. 12.7.2 R calculates everything R is a scripting language which means everything you write will be evaluated when the code will run. Compiler optimizations dont work in R. Take an example. year_2_sec &lt;- function(x){ y &lt;- x * 365 * 24 * 60 * 60 return(y) } year_2_sec_opti &lt;- function(x){ y &lt;- x * 31536000 return(y) } microbenchmark::microbenchmark( norm = year_2_sec(1:2e5), optimized = year_2_sec_opti(1:2e5) ) ## Unit: microseconds ## expr min lq mean median uq max neval cld ## norm 490.201 509.801 651.9611 534.501 703.3010 4776.201 100 a ## optimized 253.300 302.401 515.3840 329.151 497.0515 4486.501 100 a Even though the difference is very small there is a difference none the less. It goes for ( parenthesis and { braces too In R this things are functions and are actually evaluated before a result is displayed. Lets use the same example above but with many parenthesis. microbenchmark::microbenchmark( without_braces = year_2_sec_opti(1:2e5), with_braces = year_2_sec_opti({ (((((((((((((((((((1:2e5))))))))))))))))))) }) ) ## Unit: microseconds ## expr min lq mean median uq max neval cld ## without_braces 275.101 301.7515 460.0310 339.1005 482.851 4502.301 100 a ## with_braces 272.301 300.9005 465.4359 324.8505 493.100 4599.501 100 a R is not so slow language anymore so these differences are very small to notice but you can make it a habit to use only minimum code needed. This helps. Fun fact: you can only nest a function up to 50 levels after that it breaks. I have tried it before and counted numbers. Dont ask me why 12.7.3 .Internal functions R has some functions that are internal and can be accessed directly. R mostly provides you wrapper around those. Lets take an example integer ## function (length = 0L) ## .Internal(vector(&quot;integer&quot;, length)) ## &lt;bytecode: 0x0000000012dba1e0&gt; ## &lt;environment: namespace:base&gt; Here as you can read R is called an internal method by name vector and passing arguments to it. We can directly use this function as well. x &lt;- .Internal(vector(&quot;integer&quot;, 10)) y &lt;- integer(10) all.equal(x, y) ## [1] TRUE This can come in handy specially in cases where Base R is performing multiple checks. I can give you examples to do this. But I would recommend you to not use .Internal function directly until you are very sure what are you doing. In most cases you can find a faster function in some other package that you can use. 12.7.4 Dont Compile R has a package called compiler that is used to compile any function in R. In old blogs you will still see examples of compilation making your code a little faster. After R 3.5 compilation was on by default. Now every function that you create is compiled and thus it is already optimized and running a compilation on it will not give you any additional speed. However R compilers still need to be optimized and there are people working on it. Hopefully we will get a better and faster R within a few more years. You can watch a video on it if you want to know more. &lt;iframe width=560 height=315 src=https://www.youtube.com/embed/VdD0nHbcyk4%22 title=YouTube video player\" frameborder=0 allow=accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture allowfullscreen&gt;&lt;/iframe&gt; 12.7.5 use direct method.object structure R spends a little amount trying to figure out the class of the method belongs to. Like here methods(generic.function = &quot;as.Date&quot;) ## [1] as.Date.character as.Date.default as.Date.factor ## [4] as.Date.IDate* as.Date.numeric as.Date.POSIXct ## [7] as.Date.POSIXlt as.Date.vctrs_sclr* as.Date.vctrs_vctr* ## see &#39;?methods&#39; for accessing help and source code Now as you can see based on the type of object you supply as.Date will decide which method to implement. We can speed up our code a bit by directly specifying what objects are we working on. microbenchmark::microbenchmark( oops = as.Date.numeric(10000, origin = as.Date.character(&quot;1970-01-01&quot;)), norm = as.Date(10000, origin = as.Date(&quot;1970-01-01&quot;)) ) ## Unit: microseconds ## expr min lq mean median uq max neval cld ## oops 32.201 33.4015 35.95692 35.3515 37.1010 62.701 100 a ## norm 35.602 37.3010 42.13296 39.1505 40.9015 249.602 100 b It might not matter much in a single call but in a loop we can definitely use all these tricks to speed up our code. bad_code &lt;- function(x){ y &lt;- numeric(x) for(i in 1:x){ y[i] &lt;- as.Date( x = (10000 + (i)), origin = (as.Date(x = (&quot;1970-01-01&quot;))) ) } return(y) } faster_code &lt;- function(x){ y &lt;- numeric(x) for(i in 1:x){ y[[i]] &lt;- as.Date.numeric( x = 10000 + i, origin = as.Date.character(x = &quot;1970-01-01&quot;) ) } return(y) } microbenchmark::microbenchmark( bad_code(1e4), faster_code(1e4), times = 10 ) ## Unit: milliseconds ## expr min lq mean median uq max neval ## bad_code(10000) 409.6713 414.8415 432.8392 422.8580 428.7615 535.8357 10 ## faster_code(10000) 365.1592 368.2884 373.1036 371.3434 378.3060 383.2421 10 ## cld ## b ## a as you can see we can extract a few more drops from our CPU if we use these small techniques. It might not be much but again its not too much to remember. And if you use them precisely you can save seconds off an entire app by saving milliseconds on each operation. 12.8 Export Other languages There is a limitation on How fast R can go. R is not the fastest language on the world. It might be one of the slowest for sure. But we use it because of the ecosystem it provides. The ability to download packages from CRAN and the assurance that goes along with it is exceptional. No other programming language comes even close to it. And almost anything you want to do with your data can be done from R directly. Statistics, analysis, visualizations, big data, ML and DL there are packages to deal with all of it. This is the reason We mostly use R because of its ecosystem not because of speed. But when you have used most of the techniques and it still doesnt work out. You can use JAVA, C++, C, fortran, python or Julia and import those functions directly it R. R has an interface for calling all of the mentioned languages. The most simple one among all those is rcpp. Which makes C++ work like an R code. Fully vectorized and thus making it easier for an R function to be rewritten in C++ very easily. C++ is a huge language and it will take years of expertise to write c++ code. While basic Rcpp can be learned within a very small time frame and can be used effectively. My advice would be to learn basic c++ from any youtube video within like 2 to 3 hours and then read this book. https://teuder.github.io/rcpp4everyone_en/ 12.9 Conclusion In this chapter we discussed how to speed up R code with basic tips available that you can remember easily while writing code. R improves itself after every iteration use latest R Profile your code to see the slower part Benchmark your solution to check the speed Algorithm matters more than the language Always read the function to see what it is doing and do you need all this Use if statements to do as little computation as possible There is always a faster function from a package available search it use [[ when you can dont write extra functions like (,{ or anything as such to save some time .Internal functions can be used but dont use them you can find faster versions in Other packages R compiles its code by default compiler will not help you anymore If you know the exact implementation use method.object syntax Write Rcpp if necessary "],["loops.html", "Chapter 13 For Loops 13.1 initialize objects before loops 13.2 use simple data-types 13.3 apply family 13.4 Vectorize your code 13.5 Do very little inside loops", " Chapter 13 For Loops This is a topic that I wanted to discuss for a long time. People read blogs from 2014-2016 and assume that for loops in R are bad. You should not use them and Loops in R are slow etc. etc This chapter will help you understand how to use them more effectively. R loops are not too slow compared to other programming languages like python, ruby etc But Yes they are slower than the vectorized code. You can get a faster code by doing more with vectorization than a simple loop. 13.1 initialize objects before loops create vectors for storing object even before the loop starts. Because it allocates memory before the loop it makes R loop a lot faster. And creating a vector is a vectorized C function call thus its always a lot faster. R has a few functions to create the type of vector of vector you need.integer, numeric, character, logical are most common function for these cases. numeric can be used to store Date types as well. Its always beneficial to start with a vector to store the values. 13.2 use simple data-types Data-types are the most common reason people dont get speed in R. If you run a loop on a Data.frame it always have to check the constraints of a data.frame like same length vectors to make sure you are not messing up the data type and it also creates a copy on each modification. But same code could be like a 1000 times faster if we just use a simple list. R data.table packages provides an interface to set values inside a data-table without creating a copy which makes it faster for most of the use cases. Lets compare how fast will it be. set_dt_num &lt;- function(dt){ for(i in 1:1e3){ data.table::set( x = dt, i = i, j = 1L, value = i*2L ) } } set_dt_col &lt;- function(dt){ for(i in 1:1e3){ data.table::set( x = dt, i = i, j = &quot;x&quot;, value = i*2 ) } } gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 2453060 131.1 4484745 239.6 4484745 239.6 ## Vcells 4358187 33.3 19256804 147.0 165490260 1262.6 data_table &lt;- data.table::data.table(x = integer(1e3)) data_frame &lt;- data.frame(x = integer(1e3)) microbenchmark::microbenchmark( set_df_col = { for(i in 1:1e3){ data_frame$x[[i]] &lt;- i*2L } }, set_dt_num = set_dt_num(data_table), set_dt_col = set_dt_col(data_table), times = 10 ) ## Unit: milliseconds ## expr min lq mean median uq max neval cld ## set_df_col 6.290001 6.435001 6.741341 6.557052 6.921602 7.772801 10 a ## set_dt_num 6.555701 6.728101 7.118461 7.010952 7.286201 8.453201 10 a ## set_dt_col 7.177401 7.472601 8.076611 7.894751 7.945400 10.593200 10 b 13.3 apply family 13.4 Vectorize your code R is vectorized to the core. Every function in R is vectorized. Even the comparison operators are vectorized. This is a core strength of R. If you can break your task down to vectorized operation you can make it faster even after adding more steps to it. Lets take an example. dummy_text &lt;- sample( x = letters, size = 1e3, replace = TRUE ) dummy_category &lt;- sample( x = c(1,2,3), size = 1e3, replace = TRUE ) main_table &lt;- data.frame(dummy_text, dummy_category) Now this table has a 1000 text that I want to join into a a huge corpus based on their category. Anybody familiar with other programming languages like python or java or c++ will look for a loop that can solve it. If you try that approach it might go like this. join_text_norm &lt;- function(df = main_table){ text &lt;- character(length(unique(df$dummy_category))) for(i in seq_along(df$dummy_category)){ if ( df$dummy_category[[i]] == 1 ) { text[[1]] &lt;- paste0(text[[1]], df$dummy_text[[i]]) } else if ( df$dummy_category[[i]] == 2 ) { text[[2]] &lt;- paste0(text[[2]], df$dummy_text[[i]]) } else { text[[3]] &lt;- paste0(text[[3]], df$dummy_text[[i]]) } } return(text) } join_text_norm() ## [1] &quot;jpxpbodgqiwuzibakvuszmuytrtuwtgjzwudolmfriogeorfmprwtxzuiqdtmtpmyzldkfrzijpmzmwxfrqbcimwxstyevmvwasxgakaqelzubblnvyilrdqsuomtegwlxitfzqwbqrsqewficblimnvnvylygzsttlafktubhnnjdcpkajormxiznpxxqhwwlmurfsxnwpscyqfumzomxhjqbydjyutlsakumywrgcasxnqdjtssnrmvdulnwmrjhphdvzkezycuyqqkymjmmijinnmpyrwuupmknr&quot; ## [2] &quot;nspdqzjluiwjqynmicgchjmobigykuyhqhuhhdodcuibcpeokoawchfbgscaxorykfkqlsgwyknyrpzvubdyrqdpvyqnqbhulpazzchwuxjcanfvzxastgmdgaieevwrjpkrntsetgzbgawhpjkubegysxtbadrvuqjdmziybutmwflyelzwnrzhjijjtizocmstmfxyoudspemzjtsmehbbqloqgadjcgvhcdhtqtvxhkzeclpzaxvbibifnqpneofqjqjwvvmoskrzdlcnzbkolklfjybnrrjmhntsobstqwyznylfmakyvsdrralbrpvvuqihecluxcnqizemmaugxzufzywqnfaoaoxhmkvfegguucl&quot; ## [3] &quot;jkxeepkgbrxlbxtvskhjncdxrbgrgfixtjkuefgrbmzjaqqmopmdgywzfffmcbygsqeigsimutceslsqybpshqldvnjxqmhgflizfvthnseiwfwjrfssiggyhjbpsatvlvzyvqcrllrehqfvwwkzuyhfasdyzimitwkhfxwgvgeuzhniznptzwyifjljghtyduszbpchcpjirmmjyjbrirdvjhcqwqgkkvghunlbtqhaxzlhwfwwymtnusmcujjutdckpskulqyfimcporweiqwkdrjhcyuxmwidccouukleqemghlwwrfcshrhaknqbpzkfrewpxfurxz&quot; This is not the most optimized function but this can get the job done. And I am breaking a golden rule here. 13.4.1 never repeat a calculation in the above code I could save the some time by storing the value of text into a variable and stop R from calculating it again and again. join_text_saved &lt;- function( df = main_table ){ text &lt;- character(length(unique(df$dummy_category))) for(i in seq_along(df$dummy_category)){ curr_text &lt;- df$dummy_text[[i]] curr_cat &lt;- df$dummy_category[[i]] if (curr_cat == 1 ) { text[[1]] &lt;- paste0(text[[1]], curr_text) } else if ( curr_cat == 2 ) { text[[2]] &lt;- paste0(text[[2]], curr_text) } else { text[[3]] &lt;- paste0(text[[3]], curr_text) } } return(text) } join_text_saved() ## [1] &quot;jpxpbodgqiwuzibakvuszmuytrtuwtgjzwudolmfriogeorfmprwtxzuiqdtmtpmyzldkfrzijpmzmwxfrqbcimwxstyevmvwasxgakaqelzubblnvyilrdqsuomtegwlxitfzqwbqrsqewficblimnvnvylygzsttlafktubhnnjdcpkajormxiznpxxqhwwlmurfsxnwpscyqfumzomxhjqbydjyutlsakumywrgcasxnqdjtssnrmvdulnwmrjhphdvzkezycuyqqkymjmmijinnmpyrwuupmknr&quot; ## [2] &quot;nspdqzjluiwjqynmicgchjmobigykuyhqhuhhdodcuibcpeokoawchfbgscaxorykfkqlsgwyknyrpzvubdyrqdpvyqnqbhulpazzchwuxjcanfvzxastgmdgaieevwrjpkrntsetgzbgawhpjkubegysxtbadrvuqjdmziybutmwflyelzwnrzhjijjtizocmstmfxyoudspemzjtsmehbbqloqgadjcgvhcdhtqtvxhkzeclpzaxvbibifnqpneofqjqjwvvmoskrzdlcnzbkolklfjybnrrjmhntsobstqwyznylfmakyvsdrralbrpvvuqihecluxcnqizemmaugxzufzywqnfaoaoxhmkvfegguucl&quot; ## [3] &quot;jkxeepkgbrxlbxtvskhjncdxrbgrgfixtjkuefgrbmzjaqqmopmdgywzfffmcbygsqeigsimutceslsqybpshqldvnjxqmhgflizfvthnseiwfwjrfssiggyhjbpsatvlvzyvqcrllrehqfvwwkzuyhfasdyzimitwkhfxwgvgeuzhniznptzwyifjljghtyduszbpchcpjirmmjyjbrirdvjhcqwqgkkvghunlbtqhaxzlhwfwwymtnusmcujjutdckpskulqyfimcporweiqwkdrjhcyuxmwidccouukleqemghlwwrfcshrhaknqbpzkfrewpxfurxz&quot; microbenchmark::microbenchmark( join_text_norm(df = main_table), join_text_saved(df = main_table) ) ## Unit: milliseconds ## expr min lq mean median uq ## join_text_norm(df = main_table) 4.435401 4.788500 5.078652 4.987451 5.097651 ## join_text_saved(df = main_table) 3.926301 4.386152 4.570039 4.555951 4.653600 ## max neval cld ## 8.252800 100 b ## 7.851901 100 a We did not save much on it but we still saved one millisecond on just a 1000 loop. Its an excellent practice of not to repeat calculation. Now coming back to the point. You could try this approach just like every other programming language does. Or you can try a vectorized approach with the built in paste function with collapse argument. collapsed_fun &lt;- function( df = main_table ){ text &lt;- df %&gt;% split(f = dummy_category) %&gt;% lapply(function(x) paste0(x$dummy_text,collapse = &quot;&quot;) ) %&gt;% unlist() return(text) } collapsed_fun(main_table) ## 1 ## &quot;jpxpbodgqiwuzibakvuszmuytrtuwtgjzwudolmfriogeorfmprwtxzuiqdtmtpmyzldkfrzijpmzmwxfrqbcimwxstyevmvwasxgakaqelzubblnvyilrdqsuomtegwlxitfzqwbqrsqewficblimnvnvylygzsttlafktubhnnjdcpkajormxiznpxxqhwwlmurfsxnwpscyqfumzomxhjqbydjyutlsakumywrgcasxnqdjtssnrmvdulnwmrjhphdvzkezycuyqqkymjmmijinnmpyrwuupmknr&quot; ## 2 ## &quot;nspdqzjluiwjqynmicgchjmobigykuyhqhuhhdodcuibcpeokoawchfbgscaxorykfkqlsgwyknyrpzvubdyrqdpvyqnqbhulpazzchwuxjcanfvzxastgmdgaieevwrjpkrntsetgzbgawhpjkubegysxtbadrvuqjdmziybutmwflyelzwnrzhjijjtizocmstmfxyoudspemzjtsmehbbqloqgadjcgvhcdhtqtvxhkzeclpzaxvbibifnqpneofqjqjwvvmoskrzdlcnzbkolklfjybnrrjmhntsobstqwyznylfmakyvsdrralbrpvvuqihecluxcnqizemmaugxzufzywqnfaoaoxhmkvfegguucl&quot; ## 3 ## &quot;jkxeepkgbrxlbxtvskhjncdxrbgrgfixtjkuefgrbmzjaqqmopmdgywzfffmcbygsqeigsimutceslsqybpshqldvnjxqmhgflizfvthnseiwfwjrfssiggyhjbpsatvlvzyvqcrllrehqfvwwkzuyhfasdyzimitwkhfxwgvgeuzhniznptzwyifjljghtyduszbpchcpjirmmjyjbrirdvjhcqwqgkkvghunlbtqhaxzlhwfwwymtnusmcujjutdckpskulqyfimcporweiqwkdrjhcyuxmwidccouukleqemghlwwrfcshrhaknqbpzkfrewpxfurxz&quot; Lets compare it with the loop approach. microbenchmark::microbenchmark( join_text_norm(), join_text_saved(), collapsed_fun() ) ## Unit: microseconds ## expr min lq mean median uq max neval ## join_text_norm() 4300.501 4606.401 4812.826 4743.851 4874.800 8040.401 100 ## join_text_saved() 3947.800 4220.751 4463.610 4310.752 4454.751 7846.101 100 ## collapsed_fun() 906.601 1028.401 1107.853 1064.452 1121.051 4557.602 100 ## cld ## c ## b ## a Collapsed function is faster than all the other approach for just 1000 loops. Imagine doing it for 1 million. Vectorized function in those cases will be 1000 times faster than loops. 13.5 Do very little inside loops "],["multithreading.html", "Chapter 14 Multithreading", " Chapter 14 Multithreading "],["shinyspeed.html", "Chapter 15 Speed", " Chapter 15 Speed "],["shinymemory.html", "Chapter 16 Memory", " Chapter 16 Memory "]]
