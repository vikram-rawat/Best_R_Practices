# For Loops {#loops}

This is a topic that I wanted to discuss for a long time. People read blogs from 2014-2016 and assume that for loops in R are bad. You should not use them and Loops in R are slow etc. etc... This chapter will help you understand how to use them more effectively.

R loops are not too slow compared to other programming languages like python, ruby etc... But Yes they are slower than the vectorized code. You can get a faster code by doing more with vectorization than a simple loop.

## initialize objects before loops

create vectors for storing object even before the loop starts. Because it allocates memory before the loop it makes R loop a lot faster. And creating a vector is a vectorized C function call thus it's always a lot faster.

R has a few functions to create the type of vector of vector you need.`integer, numeric, character, logical` are most common function for these cases. numeric can be used to store `Date types` as well. It's always beneficial to start with a vector to store the values.

## use simple data-types

Data-types are the most common reason people don't get speed in R. If you run a loop on a Data.frame it always have to check the constraints of a data.frame like same length vectors to make sure you are not messing up the data type and it also creates a copy on each modification. But same code could be like a 1000 times faster if we just use a simple list.

R data.table packages provides an interface to set values inside a data-table without creating a copy which makes it faster for most of the use cases. Let's compare how fast will it be.

```{r}

set_dt_num <- function(dt){
  for(i in 1:1e3){
    data.table::set(
      x = dt,
      i = i,
      j = 1L,
      value = i*2L
    )
  }
}

set_dt_col <- function(dt){
  for(i in 1:1e3){
    data.table::set(
      x = dt,
      i = i,
      j = "x",
      value = i*2
    )
  }
}

gc()

data_table <- data.table::data.table(x = integer(1e3))
data_frame <- data.frame(x = integer(1e3))

microbenchmark::microbenchmark(
  set_df_col = {
    for(i in 1:1e3){
      data_frame$x[[i]] <- i*2L
    }
  },
  set_dt_num = set_dt_num(data_table),
  set_dt_col = set_dt_col(data_table),
  times = 10
)

```

## apply family

## Vectorize your code

R is vectorized to the core. Every function in R is vectorized. Even the comparison operators are vectorized. This is a core strength of R. If you can break your task down to vectorized operation you can make it faster even after adding more steps to it. Let's take an example.

```{r}

dummy_text <- sample(
  x = letters,
  size = 1e3,
  replace = TRUE
)

dummy_category <- sample(
  x = c(1,2,3),
  size = 1e3,
  replace = TRUE
)

main_table <- data.frame(dummy_text, dummy_category)
```

Now this table has a 1000 text that I want to join into a a huge corpus based on their category. Anybody familiar with other programming languages like python or java or c++ will look for a loop that can solve it. If you try that approach it might go like this.

```{r}

join_text_norm <- function(df = main_table){
  
  text <- character(length(unique(df$dummy_category)))
  
  for(i in seq_along(df$dummy_category)){
    if ( df$dummy_category[[i]] == 1 ) {
        text[[1]] <- paste0(text[[1]], df$dummy_text[[i]])
    } else   if ( df$dummy_category[[i]] == 2 ) {
        text[[2]] <- paste0(text[[2]], df$dummy_text[[i]])
    } else {
      text[[3]] <- paste0(text[[3]], df$dummy_text[[i]])
    }
  }
  
  return(text)
  
}

join_text_norm()
```

This is not the most optimized function but this can get the job done. And I am breaking a golden rule here.

### never repeat a calculation

in the above code I could save the some time by storing the value of text into a variable and stop R from calculating it again and again.

```{r}

join_text_saved <- function(
  df = main_table
  ){
  
  text <- character(length(unique(df$dummy_category)))

  for(i in seq_along(df$dummy_category)){
    curr_text <- df$dummy_text[[i]]
    curr_cat <- df$dummy_category[[i]]
    
    if (curr_cat  == 1 ) {
        text[[1]] <- paste0(text[[1]], curr_text)
    } else   if ( curr_cat == 2 ) {
        text[[2]] <- paste0(text[[2]], curr_text)
    } else {
      text[[3]] <- paste0(text[[3]], curr_text)
    }
  }
  
  return(text)
}

join_text_saved()
```

```{r}
microbenchmark::microbenchmark(
  join_text_norm(df = main_table),
  join_text_saved(df = main_table)
)
```

We did not save much on it but we still saved one millisecond on just a 1000 loop. It's an excellent practice of not to repeat calculation.

Now coming back to the point. You could try this approach just like every other programming language does. Or you can try a vectorized approach with the built in paste function with collapse argument.

```{r}

collapsed_fun <- function(
  df =  main_table
  ){
  text <- df %>% 
  split(f = dummy_category) %>% 
  lapply(function(x)
    paste0(x$dummy_text,collapse = "")
    ) %>% 
  unlist()
  
  return(text)
}

collapsed_fun(main_table)
```

Let's compare it with the loop approach.

```{r}

microbenchmark::microbenchmark(
  join_text_norm(),
  join_text_saved(),
  collapsed_fun()
)
```

Collapsed function is faster than all the other approach for just 1000 loops. Imagine doing it for 1 million. Vectorized function in those cases will be 1000 times faster than loops.

## Do very little inside loops
